{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main import block\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import cfscrape \n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from datetime import date, datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "import re\n",
    "\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Upload to google sheets\n",
    "import gspread\n",
    "#import df2gspread as d2g\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Upload to yandex\n",
    "import yadisk\n",
    "\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functions to maintain side manipulations #####\n",
    "\n",
    "\n",
    "# Timeout class for reattempting connection\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "\n",
    "# Function to convert date from json\n",
    "def date_format(date_raw):\n",
    "    timestamp = date_raw / 1000\n",
    "    date = datetime.fromtimestamp(timestamp)\n",
    "    formatted_date = date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return formatted_date\n",
    "\n",
    "\n",
    "def date_format_reverse():\n",
    "    date_now = date.today()\n",
    "    date_string = date_now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    date_raw = datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S\").timestamp()\n",
    "    date_raw = date_raw * 1000\n",
    "    return int(date_raw)\n",
    "\n",
    "\n",
    "# Function for uploading dataframes into the google docs\n",
    "def google_upload(df, sheet_name):\n",
    "    # Params used to connect to google api\n",
    "    scope = [\n",
    "        \"https://spreadsheets.google.com/feeds\",\n",
    "        \"https://www.googleapis.com/auth/drive\",\n",
    "    ]\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        \"./misc/macro-parser-lme-c2f2972b48fc.json\", scope\n",
    "    )  # security token\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    # Key params for connection to particular document\n",
    "    spreadsheet_key = \"1WhLiXRcdlkG7NCvHac9unC8ROt4lcbY7GxrOEdezZ9s\"  # document id\n",
    "    wks_name = sheet_name  # sheet name that we use\n",
    "    df_array = df.to_numpy()\n",
    "    df_new = pd.DataFrame(df_array, columns=df.columns)\n",
    "    # df_new.reset_index(drop=True, inplace=True)\n",
    "    d2g.upload(df_new, spreadsheet_key, wks_name, credentials=credentials)\n",
    "    # print(f'Uploading to {sheet_name} completed')\n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "# yandex upload\n",
    "def yandex_upload():\n",
    "    token = pd.read_csv(\"./misc/token.csv\", header=None)\n",
    "    token = token.iloc[0, 0]\n",
    "\n",
    "    y = yadisk.YaDisk(token=token)\n",
    "\n",
    "    try:\n",
    "        y.upload(\"./data/LME_db.xlsx\", \"/macro_db/LME_db.xlsx\")\n",
    "    except:\n",
    "        y.remove(\"/macro_db/LME_db.xlsx\")\n",
    "        y.upload(\"./data/LME_db.xlsx\", \"/macro_db/LME_db.xlsx\")\n",
    "\n",
    "\n",
    "# Session creation via proxy\n",
    "def get_session(url):\n",
    "\n",
    "    # Free proxy function\n",
    "    def get_free_proxies():\n",
    "        url = \"https://free-proxy-list.net/\"\n",
    "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "        raw_list = []\n",
    "        proxies = dict()\n",
    "        trs = soup.find(\"table\").find_all(\"tr\")  # main table\n",
    "\n",
    "        for i in trs[1:]:\n",
    "            raw_list.append(i.find_all(\"td\"))  # list of raw data rows\n",
    "\n",
    "        for i in range(len(raw_list)):  # creating working proxy list\n",
    "            try:\n",
    "                if raw_list[i][6].text == \"yes\":  # taking only https\n",
    "                    proxies[raw_list[i][3].text] = (\n",
    "                        f\"{raw_list[i][0].text}:{raw_list[i][1].text}\"\n",
    "                    )\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "        adress = pd.Series(proxies)  # creating proxy series\n",
    "\n",
    "        return adress\n",
    "\n",
    "    # create session\n",
    "    session = requests.Session()\n",
    "\n",
    "    # random proxy\n",
    "    proxy = get_free_proxies()\n",
    "    counter = 0\n",
    "\n",
    "    while counter <= len(proxy):\n",
    "        try:\n",
    "            with time_limit(7):\n",
    "                random_proxy = proxy.sample().values[0]\n",
    "                session.proxies = {\"https\": random_proxy}\n",
    "                response = session.get(url)\n",
    "                break\n",
    "\n",
    "        except OSError:\n",
    "            counter += 1\n",
    "            pass\n",
    "\n",
    "        except TimeoutException:\n",
    "            # print(\"NBK_tenge timed out! Another attempt\")\n",
    "            counter += 1\n",
    "            print(f\"Attempt {counter+1}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free proxy function\n",
    "def get_free_proxies():\n",
    "    url = \"https://free-proxy-list.net/\"\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "    raw_list = []\n",
    "    proxies = dict()\n",
    "    trs = soup.find(\"table\").find_all(\"tr\")  # main table\n",
    "\n",
    "    for i in trs[1:]:\n",
    "        raw_list.append(i.find_all(\"td\"))  # list of raw data rows\n",
    "\n",
    "    for i in range(len(raw_list)):  # creating working proxy list\n",
    "        try:\n",
    "            if raw_list[i][6].text == \"yes\":  # taking only https\n",
    "                proxies[raw_list[i][3].text] = (\n",
    "                    f\"{raw_list[i][0].text}:{raw_list[i][1].text}\"\n",
    "                )\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "    adress = pd.Series(proxies)  # creating proxy series\n",
    "\n",
    "    return adress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_free_proxies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hong Kong        124.156.163.166:3128\n",
       "Germany              5.9.211.233:8118\n",
       "Singapore             8.219.97.248:80\n",
       "Brazil            200.174.198.95:8888\n",
       "Canada             72.10.160.90:20931\n",
       "India               152.67.9.179:8100\n",
       "Japan               20.44.188.17:3129\n",
       "Mexico            189.240.60.169:9090\n",
       "United States      34.82.217.181:5555\n",
       "Netherlands       51.145.176.250:8080\n",
       "Finland           135.181.226.41:3128\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'124.156.163.166:3128'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{test[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "##########################   LME ###############################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def lme_selenium():\n",
    "    # Получам series proxies\n",
    "    def get_free_proxies():\n",
    "        url = \"https://free-proxy-list.net/\"\n",
    "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "        raw_list = []\n",
    "        proxies = dict()\n",
    "        trs = soup.find(\"table\").find_all(\"tr\")  # main table\n",
    "\n",
    "        for i in trs[1:]:\n",
    "            raw_list.append(i.find_all(\"td\"))  # list of raw data rows\n",
    "\n",
    "        for i in range(len(raw_list)):  # creating working proxy list\n",
    "            try:\n",
    "                if raw_list[i][6].text == \"yes\":  # taking only https\n",
    "                    proxies[raw_list[i][3].text] = (\n",
    "                        f\"{raw_list[i][0].text}:{raw_list[i][1].text}\"\n",
    "                    )\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "        adress = pd.Series(proxies)  # creating proxy series\n",
    "\n",
    "        return adress\n",
    "\n",
    "    # Создаем лист с проксямя\n",
    "    proxy_list = get_free_proxies()\n",
    "\n",
    "    # Функция на получение дневной инфы\n",
    "    def get_price(metall=\"\"):\n",
    "        url = f\"https://www.lme.com/en/Metals/Non-ferrous/LME-{metall}#Summary\"\n",
    "\n",
    "        prices = {}\n",
    "\n",
    "        # Пройдемся по проксям\n",
    "        for i in range(len(proxy_list)):\n",
    "            proxy = Proxy(\n",
    "                {\n",
    "                    \"proxyType\": ProxyType.MANUAL,\n",
    "                    \"httpProxy\": f\"{proxy_list[i]}\",\n",
    "                    \"sslProxy\": f\"{proxy_list[i]}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(f\"--proxy-server=http://{proxy_list[i]}\")\n",
    "\n",
    "            service = Service()\n",
    "\n",
    "            driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "            try:\n",
    "                # driver.maximize_window()\n",
    "                driver.get(url=url)\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "                driver.execute_script(\n",
    "                    \"window.scrollTo(0, window.scrollY + window.innerHeight);\"\n",
    "                )\n",
    "                driver.execute_script(\n",
    "                    \"window.scrollTo(0, window.scrollY + window.innerHeight);\"\n",
    "                )\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "                html_code = driver.page_source\n",
    "\n",
    "                soup = BeautifulSoup(html_code, \"html.parser\")\n",
    "                data_raw = soup.find_all(\"td\", class_=\"data-set-table__cell\")\n",
    "                price = pd.to_numeric(data_raw[1].text)\n",
    "                prices[metall] = price\n",
    "                break\n",
    "\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                continue\n",
    "\n",
    "            finally:\n",
    "                driver.close()\n",
    "                driver.quit()\n",
    "                return prices\n",
    "\n",
    "    al_price = get_price(\"Aluminium\")\n",
    "    cu_price = get_price(\"Copper\")\n",
    "    zn_price = get_price(\"Zinc\")\n",
    "    nk_price = get_price(\"Nickel\")\n",
    "    ld_price = get_price(\"Lead\")\n",
    "\n",
    "    '''data_row = pd.DataFrame(\n",
    "        {\n",
    "            \"aluminium\": al_price,\n",
    "            \"copper\": cu_price,\n",
    "            \"lead\": ld_price,\n",
    "            \"nickel\": nk_price,\n",
    "            \"zink\": zn_price,\n",
    "        },\n",
    "        index=[0],\n",
    "    )'''\n",
    "    \n",
    "    data_row = pd.DataFrame({\n",
    "        \"aluminium\": al_price,\n",
    "        \"copper\": cu_price,\n",
    "        \"lead\": ld_price,\n",
    "        \"nickel\": nk_price,\n",
    "        \"zink\": zn_price,\n",
    "        },index=[0],)\n",
    "    \n",
    "\n",
    "    return data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lme_selenium()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
