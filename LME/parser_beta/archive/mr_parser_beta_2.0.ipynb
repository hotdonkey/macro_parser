{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LME parsing is DONE!\n",
      "Uploading to LME_non_ferrous completed\n",
      "KITCO parsing is DONE!\n",
      "Uploading to KITCO completed\n",
      "CentroBank_currency parsing is DONE!\n",
      "Uploading to cb_curr completed\n",
      "CentroBank_metalls parsing is DONE!\n",
      "Uploading to cb_metall completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "# This block is using for upload to google sheets\n",
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "\n",
    "# Function for uploading dataframes into the google docs\n",
    "def google_upload(df, sheet_name):\n",
    "    # Params used to connect to google api\n",
    "    scope = [\n",
    "        'https://spreadsheets.google.com/feeds',\n",
    "        'https://www.googleapis.com/auth/drive']\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        'macro-parser-lme-c2f2972b48fc.json', scope) #security token\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    #Key params for connection to particular document\n",
    "    spreadsheet_key = '1WhLiXRcdlkG7NCvHac9unC8ROt4lcbY7GxrOEdezZ9s' #document id\n",
    "    wks_name = sheet_name  # sheet name that we use\n",
    "    d2g.upload(df, spreadsheet_key, wks_name, credentials=credentials)\n",
    "    print(f'Uploading to {sheet_name} completed')\n",
    "\n",
    "\n",
    "def lme_db_addition():\n",
    "\n",
    "    def get_day_info():\n",
    "\n",
    "        # URL API for every metall, place into the var\n",
    "        url_aluminium = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=1a0ef0b6-3ee6-4e44-a415-7a313d5bd771'\n",
    "        url_copper = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=762a3883-b0e1-4c18-b34b-fe97a1f2d3a5'\n",
    "        url_lead = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=bc443de6-0bdd-4464-8845-9504f528b0c6'\n",
    "        url_nikel = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=acadf037-c13f-42f2-b42a-cac9a8179940'\n",
    "        url_zink = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=c389e2b0-c4a3-46a0-96ca-69cacbe90ee4'\n",
    "\n",
    "        # List for iterations\n",
    "        req_list = {'aluminium': url_aluminium, 'copper': url_copper,\n",
    "                    'lead': url_lead, 'nikel': url_nikel, 'zink': url_zink}\n",
    "\n",
    "        # Empty dict for final row-stage\n",
    "        day_dict = {'date': [], 'aluminium': [], 'copper': [],\n",
    "                    'lead': [], 'nikel': [], 'zink': [], }\n",
    "\n",
    "        # Getting json from api's requests and taking the info (in our case OFFER price for a date)\n",
    "        for metal, url in req_list.items():\n",
    "            req = requests.get(url).json()\n",
    "            metal_dict = req['Rows'][0]\n",
    "            day_dict[metal] = float(metal_dict['Values'][1])\n",
    "            day_dict['date'] = metal_dict['BusinessDateTime']\n",
    "\n",
    "        # Transform dict from previous stage into the row\n",
    "        dict_ = dict(day_dict)  # Maybe can simplify this\n",
    "        day_row = pd.DataFrame([dict_])\n",
    "        day_row.date = pd.to_datetime(day_row.date)\n",
    "\n",
    "        return day_row  # This is our row for implimentation into the main base\n",
    "\n",
    "    # Opening main base, add a row, check for dupp, saving and closing\n",
    "    lme_db = pd.read_excel(\n",
    "        '../parser_beta/data/LME_db.xlsx', index_col=0)\n",
    "    day_row = get_day_info()\n",
    "    lme_db = pd.concat([lme_db, day_row], axis=0, ignore_index=True)\n",
    "    lme_db.drop_duplicates(inplace=True)\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/LME_db.xlsx\",\n",
    "        date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "        lme_db.to_excel(writer, sheet_name='LME_non_ferrous')\n",
    "    print('LME parsing is DONE!')\n",
    "\n",
    "    google_upload(lme_db, 'LME_non_ferrous')\n",
    "\n",
    "    return lme_db\n",
    "\n",
    "\n",
    "def kitco_db():\n",
    "    # In KITCO parsing we're taking slightly different aproach, we need to reupload\n",
    "    # the table into the file because sometimes KITCO changing data backdating\n",
    "    \n",
    "    #!!!Need to work out the implementation of previous year data!!!!\n",
    "    #year = int(date.today().year) - 2001\n",
    "    \n",
    "    url = 'https://www.kitco.com/gold.londonfix.html'\n",
    "    #url_previous_year = f'https://www.kitco.com/londonfix/gold.londonfix{year}.html'\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    #responce_prev = requests.get(url_previous_year, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    kitco_response = pd.read_html(response.text)\n",
    "    kitco_df = pd.read_excel(\n",
    "        '../parser_beta/data/kitko_db.xlsx', index_col=0)\n",
    "\n",
    "    # Get the raw table and drop unnesesary rows\n",
    "    kitco_day = kitco_response[1]\n",
    "    kitco_day.drop([1, 4, 6], axis=1, inplace=True)\n",
    "    kitco_day.columns = kitco_day.iloc[0]\n",
    "    kitco_day.drop([0, 1, 2], axis=0, inplace=True)\n",
    "\n",
    "    # Change tyoe of data within table\n",
    "    kitco_day['Date'] = pd.to_datetime(kitco_day['Date'])\n",
    "    kitco_day = kitco_day.replace({'-': np.nan})\n",
    "    kitco_day = kitco_day.sort_values(by=['Date'])\n",
    "    kitco_day = kitco_day.reset_index(drop=True)\n",
    "    kitco_day[['Gold', 'Silver', 'Platinum', 'Palladium']] = kitco_day[[\n",
    "        'Gold', 'Silver', 'Platinum', 'Palladium']].apply(pd.to_numeric)\n",
    "\n",
    "    kitco_day.drop_duplicates(inplace=True)\n",
    "\n",
    "    # And rewrite old table\n",
    "    kitco_day.to_excel(\n",
    "        '../parser_beta/data/kitko_db.xlsx', sheet_name='kitco_metall')\n",
    "    print('KITCO parsing is DONE!')\n",
    "\n",
    "    google_upload(kitco_day, 'KITCO')\n",
    "\n",
    "    return kitco_day\n",
    "\n",
    "\n",
    "def cb_curr():\n",
    "    day = date.today()\n",
    "    today = day.strftime('%d/%m/%Y')\n",
    "\n",
    "    dict_of_currencies = {\n",
    "        'R01235': 'USD',\n",
    "        'R01239': 'EUR',\n",
    "        'R01010': 'Australian_Dollar',\n",
    "        'R01375': 'China_Yuan',\n",
    "        'R01035': 'British_Pound',\n",
    "        'R01335': 'Kazakhstan_Tenge',\n",
    "        'R01820': 'Japanese_Yen',\n",
    "        'R01775': 'Swiss_Franc'\n",
    "    }\n",
    "\n",
    "    list_of_currencies = [x for x in dict_of_currencies.keys()]\n",
    "\n",
    "    URL_list = []\n",
    "    for currency in list_of_currencies:\n",
    "        URL = f'http://www.cbr.ru/scripts/XML_dynamic.asp?date_req1={today}&date_req2={today}&VAL_NM_RQ={currency}'\n",
    "        URL_list.append(URL)\n",
    "\n",
    "    currency_df = pd.read_excel(\n",
    "        '../parser_beta/data/cb_curr.xlsx', index_col=0)\n",
    "\n",
    "    # This problem occurs in the beginning of the year so I was forced to catch a ValueError\n",
    "    \n",
    "    try:\n",
    "        for url_element in URL_list:\n",
    "            response_df = pd.read_xml(url_element)\n",
    "            response_df['Date'] = pd.to_datetime(\n",
    "                response_df['Date'], dayfirst=True)\n",
    "            response_df['Value'] = response_df['Value'].apply(\n",
    "                lambda x: x.replace(',', '.'))\n",
    "            response_df['Value'] = response_df['Value'].apply(pd.to_numeric)\n",
    "            response_df = response_df.replace(dict_of_currencies)\n",
    "            currency_df = pd.concat(\n",
    "                [currency_df, response_df], axis=0, ignore_index=True)\n",
    "            currency_df.drop_duplicates(inplace=True)\n",
    "        print('CentroBank_currency parsing is DONE!')\n",
    "\n",
    "    except ValueError:\n",
    "        print('Empty data set in CentroBank_currency. Should check the source.')\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "            \"../parser_beta/data/cb_curr.xlsx\") as writer:\n",
    "        currency_df.to_excel(writer, sheet_name='curr')\n",
    "\n",
    "    google_upload(currency_df, 'cb_curr')\n",
    "\n",
    "    return currency_df\n",
    "\n",
    "\n",
    "def cb_metall():\n",
    "    day = date.today()\n",
    "    today = day.strftime('%d/%m/%Y')\n",
    "\n",
    "    metall_dict = {\n",
    "        1: 'gold',\n",
    "        2: 'silver',\n",
    "        3: 'platinum',\n",
    "        4: 'palladium'\n",
    "    }\n",
    "\n",
    "    URL = f'http://www.cbr.ru/scripts/xml_metall.asp?date_req1={today}&date_req2={today}'\n",
    "\n",
    "    metall_df = pd.read_excel(\n",
    "        '../parser_beta/data/cb_metall.xlsx', index_col=0)\n",
    "\n",
    "    # This problem occurs in the beginning of the year so I was forced to catch a ValueError\n",
    "    try:\n",
    "        response_df = pd.read_xml(URL)\n",
    "        response_df.drop(columns='Buy', axis=1, inplace=True)\n",
    "        response_df['Date'] = pd.to_datetime(\n",
    "            response_df['Date'], dayfirst=True)\n",
    "        response_df['Sell'] = response_df['Sell'].apply(\n",
    "            lambda x: x.replace(',', '.'))  # changing for future retyping to numeric\n",
    "        response_df['Sell'] = response_df['Sell'].apply(pd.to_numeric)\n",
    "        response_df = response_df.replace(metall_dict)\n",
    "\n",
    "        metall_df = pd.concat(\n",
    "            [metall_df, response_df],\n",
    "            axis=0,\n",
    "            ignore_index=True)\n",
    "\n",
    "        metall_df.drop_duplicates(inplace=True)\n",
    "        print('CentroBank_metalls parsing is DONE!')\n",
    "\n",
    "    except ValueError:\n",
    "        print('Empty data set in CentroBank_metalls . Should check the source.')\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "            '../parser_beta/data/cb_metall.xlsx',\n",
    "            date_format='YYYY-MM-DD',\n",
    "            datetime_format='YYYY-MM-DD') as writer:\n",
    "        metall_df.to_excel(writer, sheet_name='cb_metall')\n",
    "\n",
    "    google_upload(metall_df, 'cb_metall')\n",
    "\n",
    "    return metall_df\n",
    "\n",
    "def nbk_tenge():\n",
    "    #Realy unrelieable source, mb it would be better off with using ms query inside the file\n",
    "    year = date.today().year\n",
    "\n",
    "    upper_bound = f'01.01.{year}'\n",
    "    lower_bound = f'31.12.{year}'\n",
    "\n",
    "    url = f'https://nationalbank.kz/ru/exchangerates/ezhednevnye-oficialnye-rynochnye-kursy-valyut\\\n",
    "        /report?rates%5B%5D=5&beginDate={upper_bound}&endDate={lower_bound}'\n",
    "        \n",
    "    page = requests.get(url=url)\n",
    "    temp_df = pd.read_html(page.text)\n",
    "    df =temp_df[0]\n",
    "    df['Unnamed: 0'] = pd.to_datetime(df['Unnamed: 0'], dayfirst=True)\n",
    "    \n",
    "    with pd.ExcelWriter(\n",
    "            '../parser_beta/data/nbk_tenge.xlsx') as writer:\n",
    "        df.to_excel(writer, sheet_name='tenge')\n",
    "    \n",
    "    print('NBK_tenge parsing is DONE!')\n",
    "    \n",
    "    google_upload(df, 'nbk_tenge')\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lme_db_addition()\n",
    "    kitco_db()\n",
    "    cb_curr()\n",
    "    cb_metall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBK_tenge parsing is DONE!\n",
      "Uploading to nbk_tenge completed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    nbk_tenge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d40a6ae76423cf5fae73028028fb017d8630dd167584993d74bc99181da5a036"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
