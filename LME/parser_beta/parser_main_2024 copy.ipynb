{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main import block\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import cfscrape \n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from datetime import date, datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import re\n",
    "import asyncio\n",
    "\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Upload to google sheets\n",
    "import gspread\n",
    "#import df2gspread as d2g\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Upload to yandex\n",
    "import yadisk\n",
    "\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functions to maintain side manipulations #####\n",
    "\n",
    "\n",
    "# Timeout class for reattempting connection\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "\n",
    "# Function to convert date from json\n",
    "def date_format(date_raw):\n",
    "    timestamp = date_raw / 1000\n",
    "    date = datetime.fromtimestamp(timestamp)\n",
    "    formatted_date = date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return formatted_date\n",
    "\n",
    "\n",
    "def date_format_reverse():\n",
    "    date_now = date.today()\n",
    "    date_string = date_now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    date_raw = datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S\").timestamp()\n",
    "    date_raw = date_raw * 1000\n",
    "    return int(date_raw)\n",
    "\n",
    "\n",
    "# Function for uploading dataframes into the google docs\n",
    "def google_upload(df, sheet_name):\n",
    "    # Params used to connect to google api\n",
    "    scope = [\n",
    "        \"https://spreadsheets.google.com/feeds\",\n",
    "        \"https://www.googleapis.com/auth/drive\",\n",
    "    ]\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        \"./misc/macro-parser-lme-c2f2972b48fc.json\", scope\n",
    "    )  # security token\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    # Key params for connection to particular document\n",
    "    spreadsheet_key = \"1WhLiXRcdlkG7NCvHac9unC8ROt4lcbY7GxrOEdezZ9s\"  # document id\n",
    "    wks_name = sheet_name  # sheet name that we use\n",
    "    df_array = df.to_numpy()\n",
    "    df_new = pd.DataFrame(df_array, columns=df.columns)\n",
    "    # df_new.reset_index(drop=True, inplace=True)\n",
    "    d2g.upload(df_new, spreadsheet_key, wks_name, credentials=credentials)\n",
    "    # print(f'Uploading to {sheet_name} completed')\n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "# yandex upload\n",
    "def yandex_upload():\n",
    "    token = pd.read_csv(\"./misc/token.csv\", header=None)\n",
    "    token = token.iloc[0, 0]\n",
    "\n",
    "    y = yadisk.YaDisk(token=token)\n",
    "\n",
    "    try:\n",
    "        y.upload(\"./data/LME_db.xlsx\", \"/macro_db/LME_db.xlsx\")\n",
    "    except:\n",
    "        y.remove(\"/macro_db/LME_db.xlsx\")\n",
    "        y.upload(\"./data/LME_db.xlsx\", \"/macro_db/LME_db.xlsx\")\n",
    "\n",
    "\n",
    "# Session creation via proxy\n",
    "def get_session(url):\n",
    "\n",
    "    # Free proxy function\n",
    "    def get_free_proxies():\n",
    "        url = \"https://free-proxy-list.net/\"\n",
    "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "        raw_list = []\n",
    "        proxies = dict()\n",
    "        trs = soup.find(\"table\").find_all(\"tr\")  # main table\n",
    "\n",
    "        for i in trs[1:]:\n",
    "            raw_list.append(i.find_all(\"td\"))  # list of raw data rows\n",
    "\n",
    "        for i in range(len(raw_list)):  # creating working proxy list\n",
    "            try:\n",
    "                if raw_list[i][6].text == \"yes\":  # taking only https\n",
    "                    proxies[raw_list[i][3].text] = (\n",
    "                        f\"{raw_list[i][0].text}:{raw_list[i][1].text}\"\n",
    "                    )\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "        adress = pd.Series(proxies)  # creating proxy series\n",
    "\n",
    "        return adress\n",
    "\n",
    "    # create session\n",
    "    session = requests.Session()\n",
    "\n",
    "    # random proxy\n",
    "    proxy = get_free_proxies()\n",
    "    counter = 0\n",
    "\n",
    "    while counter <= len(proxy):\n",
    "        try:\n",
    "            with time_limit(7):\n",
    "                random_proxy = proxy.sample().values[0]\n",
    "                session.proxies = {\"https\": random_proxy}\n",
    "                response = session.get(url)\n",
    "                break\n",
    "\n",
    "        except OSError:\n",
    "            counter += 1\n",
    "            pass\n",
    "\n",
    "        except TimeoutException:\n",
    "            # print(\"NBK_tenge timed out! Another attempt\")\n",
    "            counter += 1\n",
    "            print(f\"Attempt {counter+1}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the main function block ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "##########################   LME ###############################\n",
    "################################################################\n",
    "# Асинхронная функция\n",
    "async def lme_selenium_async():\n",
    "    url = \"https://www.lme.com/Metals/Non-ferrous#tabIndex=1\"\n",
    "\n",
    "    service = Service()\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    try:\n",
    "        driver.maximize_window()\n",
    "        driver.get(url=url)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        driver.execute_script(\n",
    "            \"window.scrollTo(0, window.scrollY + window.innerHeight);\"\n",
    "        )\n",
    "        driver.execute_script(\n",
    "            \"window.scrollTo(0, window.scrollY + window.innerHeight);\"\n",
    "        )\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        html_code = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html_code, \"html.parser\")\n",
    "        # Вытаскиваем данные из табличного блока\n",
    "        data_raw = soup.find_all(\"div\", class_=\"metal-block-row__blocks\")\n",
    "\n",
    "        # Приводим к тексту\n",
    "        metalls_raw = data_raw[0].text\n",
    "\n",
    "        # Убираем все лишнее\n",
    "        metalls_raw = metalls_raw.replace(\" \", \"\").replace(\"LME\", \"LME_\").split(\"\\n\")\n",
    "        metalls_raw = \" \".join(metalls_raw).strip().split(\" \")\n",
    "\n",
    "        # Конфигурируем новый лист по металлам\n",
    "        metalls_raw = [metalls_raw[i : i + 4] for i in range(0, len(metalls_raw), 4)]\n",
    "\n",
    "        # Обрезаем до нужной инфы\n",
    "        metalls_raw = [i[:2] for i in metalls_raw]\n",
    "\n",
    "        # Приведем цены к числовому формату\n",
    "        for metall in metalls_raw:\n",
    "            metall[1] = pd.to_numeric(metall[1])\n",
    "\n",
    "        # Формируем строку датафрейма по металлам\n",
    "        dict_list = [{item[0]: item[1]} for item in metalls_raw]\n",
    "\n",
    "        metall_df = pd.DataFrame()\n",
    "\n",
    "        for i in dict_list:\n",
    "            keys = list(i.keys())\n",
    "            values = list(i.values())\n",
    "            metall_df[keys[0]] = values\n",
    "\n",
    "        # Дропаем ненужные колонки\n",
    "        metall_df.drop(columns=[\"LME_AluminiumAlloy\", \"LME_NASAAC\"], inplace=True)\n",
    "        metall_df = metall_df.rename(\n",
    "            columns={\n",
    "                \"LME_Aluminium\": \"aluminium\",\n",
    "                \"LME_Copper\": \"copper\",\n",
    "                \"LME_Lead\": \"lead\",\n",
    "                \"LME_Nickel\": \"nickel\",\n",
    "                \"LME_Zinc\": \"zink\",\n",
    "                \"LME_Tin\": \"tin\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Достанем в вставим дату\n",
    "        date_raw = soup.find_all(\"span\", class_=\"metal-block-container__refreshed-on\")\n",
    "        date_raw = date_raw[0]\n",
    "        date_raw = str(date_raw).split(\">\")[1]\n",
    "        date_raw = date_raw.replace(\"\\xa0\", \"\").replace(\"</span\", \"\")\n",
    "        date_raw = pd.to_datetime(date_raw) - pd.Timedelta(days=1)\n",
    "\n",
    "        metall_df[\"date\"] = date_raw\n",
    "\n",
    "        # Поменяем порядок колонок\n",
    "        new_column_order = [\n",
    "            \"date\",\n",
    "            \"aluminium\",\n",
    "            \"copper\",\n",
    "            \"lead\",\n",
    "            \"nickel\",\n",
    "            \"zink\",\n",
    "            \"tin\",\n",
    "        ]\n",
    "        metall_df = metall_df[new_column_order]\n",
    "\n",
    "        # Сохраняем полученные результаты\n",
    "        lme_db = pd.read_excel(\"./data/LME_db_new.xlsx\", index_col=0)\n",
    "        lme_db = pd.concat([lme_db, metall_df]).reset_index(drop=True)\n",
    "\n",
    "        # Дропаем дубликаты\n",
    "        lme_db = lme_db.drop_duplicates(subset=lme_db.columns.to_list()[1:])\n",
    "\n",
    "        # Сохраняем\n",
    "        with pd.ExcelWriter(\n",
    "            \"./data/LME_db_new.xlsx\",\n",
    "            date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\",\n",
    "        ) as writer:\n",
    "            lme_db.to_excel(writer, sheet_name=\"LME_main\")\n",
    "        \n",
    "        print(\"LME_main is done!!!\")\n",
    "\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    finally:\n",
    "        driver.close()\n",
    "        driver.quit()\n",
    "\n",
    "    return lme_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#########################   KITCO (main) ########################\n",
    "################################################################\n",
    "\n",
    "async def kitko_parser_async():\n",
    "    def kitco_raw():\n",
    "        try:\n",
    "                url = \"https://www.kitco.com/price/fixes/london-fix\"\n",
    "\n",
    "                service = Service()\n",
    "                driver = webdriver.Chrome(service=service)\n",
    "\n",
    "                # driver.maximize_window()\n",
    "                driver.get(url=url)\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "                driver.execute_script(\n",
    "                    \"window.scrollTo(0, window.scrollY + window.innerHeight);\"\n",
    "                )\n",
    "                driver.execute_script(\n",
    "                    \"window.scrollTo(0, window.scrollY + window.innerHeight);\"\n",
    "                )\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "                html_code = driver.page_source\n",
    "\n",
    "                return html_code\n",
    "\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "        finally:\n",
    "            driver.close()\n",
    "            driver.quit()\n",
    "    \n",
    "    # Работа над полученными данными\n",
    "    html_code = kitco_raw()\n",
    "    soup = BeautifulSoup(html_code)\n",
    "    \n",
    "    # Выделим данные и предобработаем для получения дневных данных\n",
    "    day = soup.find_all(\"div\", class_=\"border\")[1]\n",
    "    for i in day:\n",
    "        day = list(i.find_all(\"div\"))\n",
    "\n",
    "    day = day[4:]\n",
    "\n",
    "    day = [str(i) for i in day]\n",
    "\n",
    "    day = [i.replace(\"<div>\", \"\").replace(\"</div>\", \"\") for i in day]\n",
    "\n",
    "    day[0] = pd.to_datetime(day[0])\n",
    "\n",
    "    day[1] = day[1].split(\"/\")\n",
    "    day[1] = day[1][1].strip().replace(\",\", \"\")\n",
    "    day[1] = pd.to_numeric(day[1])\n",
    "\n",
    "    day[2] = pd.to_numeric(day[2])\n",
    "\n",
    "    day[3] = day[3].split(\"/\")\n",
    "    day[3] = day[3][1].strip().replace(\",\", \"\")\n",
    "    day[3] = pd.to_numeric(day[3])\n",
    "\n",
    "    day[4] = day[4].split(\"/\")\n",
    "    day[4] = day[4][1].strip().replace(\",\", \"\")\n",
    "    day[4] = pd.to_numeric(day[4])\n",
    "\n",
    "    df_row = (\n",
    "        pd.DataFrame([day], columns=[\"Date\", \"Gold\", \"Silver\", \"Platinum\", \"Palladium\"])\n",
    "        .sort_values(\"Date\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Выделим данные и предобработаем для получения исторических данных\n",
    "    historical = soup.find_all(\"div\", class_=\"border\")[2]\n",
    "    for i in historical:\n",
    "        historical = list(i.find_all(\"div\"))\n",
    "\n",
    "    historical = historical[5:]\n",
    "\n",
    "    historical = [str(i) for i in historical]\n",
    "\n",
    "    historical = [i.replace(\"<div>\", \"\").replace(\"</div>\", \"\") for i in historical]\n",
    "    historical = [item for item in historical if \"<div\" not in item]\n",
    "\n",
    "    # Разобьем по дням\n",
    "    historical = [historical[i : i + 5] for i in range(0, len(historical), 5)]\n",
    "\n",
    "    # Обработаем данные внутри каждого дня\n",
    "    for i in historical:\n",
    "        i[0] = pd.to_datetime(i[0])\n",
    "\n",
    "        i[1] = i[1].split(\"/\")\n",
    "        i[1] = i[1][1].strip().replace(\",\", \"\")\n",
    "        i[1] = pd.to_numeric(i[1])\n",
    "\n",
    "        i[2] = pd.to_numeric(i[2])\n",
    "\n",
    "        i[3] = i[3].split(\"/\")\n",
    "        i[3] = i[3][1].strip().replace(\",\", \"\")\n",
    "        i[3] = pd.to_numeric(i[3])\n",
    "\n",
    "        i[4] = i[4].split(\"/\")\n",
    "        i[4] = i[4][1].strip().replace(\",\", \"\")\n",
    "        i[4] = pd.to_numeric(i[4])\n",
    "\n",
    "    df_historical = (\n",
    "        pd.DataFrame(\n",
    "            historical, columns=[\"Date\", \"Gold\", \"Silver\", \"Platinum\", \"Palladium\"]\n",
    "        )\n",
    "        .sort_values(\"Date\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    df_historical = pd.concat([df_historical, df_row]).reset_index(drop=True)\n",
    "    kitko_db = pd.read_excel(\"./data/kitko_db.xlsx\", index_col=0)\n",
    "\n",
    "    kitko_db = (\n",
    "        pd.concat([kitko_db, df_historical])\n",
    "        .reset_index(drop=True)\n",
    "        .drop_duplicates(subset=[\"Date\"])\n",
    "    )\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"./data/kitko_db.xlsx\",\n",
    "        date_format=\"YYYY-MM-DD\",\n",
    "        datetime_format=\"YYYY-MM-DD\",\n",
    "    ) as writer:\n",
    "        kitko_db.to_excel(writer, sheet_name=\"kitco_metall\")\n",
    "    \n",
    "    print(\"KITKO_main is done!!!\") \n",
    "    \n",
    "    return kitko_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#########################   LBMA (KITCO subs) ##################\n",
    "################################################################\n",
    "\n",
    "async def lbma_prescious_async():\n",
    "    url_gold = \"https://prices.lbma.org.uk/json/gold_pm.json\"\n",
    "    url_silver = \"https://prices.lbma.org.uk/json/silver.json\"\n",
    "    url_platinum = \"https://prices.lbma.org.uk/json/platinum_pm.json\"\n",
    "    url_paladium = \"https://prices.lbma.org.uk/json/palladium_pm.json\"\n",
    "\n",
    "    def get_raw_data(url, metall=\"Default\"):\n",
    "        scraper = cfscrape.create_scraper()\n",
    "        scraped_data = scraper.get(url)\n",
    "\n",
    "        # scraped_data = get_session(url=url)\n",
    "\n",
    "        raw_data = pd.read_json(scraped_data.text)\n",
    "        data = raw_data[[\"d\", \"v\"]]\n",
    "        data[\"v\"] = data[\"v\"].apply(lambda x: x[0])\n",
    "        data[\"d\"] = pd.to_datetime(data[\"d\"])\n",
    "\n",
    "        data.rename(columns={\"d\": \"Date\", \"v\": metall}, inplace=True)\n",
    "        data = data.tail(4)\n",
    "\n",
    "        return data\n",
    "\n",
    "    gold = get_raw_data(url_gold, metall=\"Gold\")\n",
    "    silver = get_raw_data(url_silver, metall=\"Silver\")\n",
    "    platinum = get_raw_data(url_platinum, metall=\"Platinum\")\n",
    "    paladium = get_raw_data(url_paladium, metall=\"Palladium\")\n",
    "\n",
    "    result_df = (\n",
    "        gold.merge(silver, on=\"Date\", how=\"outer\")\n",
    "        .merge(platinum, on=\"Date\", how=\"outer\")\n",
    "        .merge(paladium, on=\"Date\", how=\"outer\")\n",
    "    )\n",
    "\n",
    "    result_df.fillna(value=0, inplace=True)\n",
    "    result_df = result_df.sort_values(by=\"Date\")\n",
    "    result_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    historical = pd.read_excel(\"./data/lbme_kitco_subs.xlsx\", index_col=0)\n",
    "\n",
    "    result = pd.concat([historical, result_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    result.drop_duplicates(inplace=True)\n",
    "\n",
    "    result = result.sort_values(by=\"Date\")\n",
    "\n",
    "    result.to_excel(\"./data/lbme_kitco_subs.xlsx\", sheet_name=\"lbme_metall\")\n",
    "    \n",
    "    print(\"LBMA is done!!!\") \n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "####################   CB_currencies ###########################\n",
    "################################################################\n",
    "\n",
    "def cb_currencies():\n",
    "    # current_year = datetime.datetime.now().year\n",
    "    current_year = datetime.now().year\n",
    "\n",
    "    dict_of_currencies = {\n",
    "        \"USD\": \"R01235\",\n",
    "        \"EUR\": \"R01239\",\n",
    "        \"Australian_Dollar\": \"R01010\",\n",
    "        \"China_Yuan\": \"R01375\",\n",
    "        \"British_Pound\": \"R01035\",\n",
    "        \"Kazakhstan_Tenge\": \"R01335\",\n",
    "        \"Japanese_Yen\": \"R01820\",\n",
    "        \"Swiss_Franc\": \"R01775\",\n",
    "    }\n",
    "\n",
    "    def get_data(currency):\n",
    "\n",
    "        url = f'https://www.cbr.ru/currency_base/dynamics/?UniDbQuery.\\\n",
    "Posted=True&UniDbQuery.so=1&UniDbQuery.mode=1&UniDbQuery.date_req1=&UniDbQuery\\\n",
    ".date_req2=&UniDbQuery.VAL_NM_RQ={currency}&UniDbQuery.From={\"01.01.2022\"}&UniDbQuery\\\n",
    ".To=31.12.{current_year}'\n",
    "\n",
    "        number_of_tries = 0\n",
    "\n",
    "        while number_of_tries < 20:\n",
    "            try:\n",
    "                scraper = cfscrape.create_scraper()\n",
    "                scraped_data = scraper.get(url)\n",
    "\n",
    "                preprocesed_data = scraped_data.text.replace(\",\", \".\")\n",
    "\n",
    "                df = pd.read_html(preprocesed_data, header=1)[0]\n",
    "\n",
    "                base_name_list = df.columns.to_list()\n",
    "                rename_list = [\"Date\", \"Nominal\", \"Value\"]\n",
    "                columns = dict(zip(base_name_list, rename_list))\n",
    "                df.rename(columns=columns, inplace=True)\n",
    "                df[\"Value\"] = df[\"Value\"] / df[\"Nominal\"]\n",
    "                df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True)\n",
    "                df.sort_values(by=\"Date\", inplace=True)\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                return df\n",
    "                break\n",
    "\n",
    "            except:\n",
    "                # print('Error occured!')\n",
    "                time.sleep(1)\n",
    "                number_of_tries += 1\n",
    "\n",
    "    for key, value in dict_of_currencies.items():\n",
    "        table = get_data(value)\n",
    "\n",
    "        db_frame = pd.read_excel(f\"./data/centrobank/{key}.xlsx\", index_col=0)\n",
    "\n",
    "        new_frame = pd.concat([table, db_frame]).drop_duplicates()\n",
    "\n",
    "        print(key, new_frame.tail())\n",
    "\n",
    "        google_upload(new_frame, f\"{key}\")\n",
    "\n",
    "        with pd.ExcelWriter(\n",
    "            f\"./data/centrobank/{key}.xlsx\",\n",
    "            date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\",\n",
    "        ) as writer:\n",
    "            new_frame.to_excel(writer, sheet_name=f\"{key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "####################   CB_metalls ##############################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def cb_metalls():\n",
    "    current_year = datetime.now().year\n",
    "\n",
    "    url = f\"https://www.cbr.ru/hd_base/metall/metall_base_new/?UniDbQuery.Posted\\\n",
    "=True&UniDbQuery.From=01.01.2022&UniDbQuery.To=30.12.{current_year}&UniDbQuery.Gold\\\n",
    "=true&UniDbQuery.Silver=true&UniDbQuery.Platinum=true&UniDbQuery.Palladium\\\n",
    "=true&UniDbQuery.so=1\"\n",
    "\n",
    "    number_of_tries = 0\n",
    "\n",
    "    while number_of_tries < 20:\n",
    "        try:\n",
    "            scraper = cfscrape.create_scraper()\n",
    "            scraped_data = scraper.get(url)\n",
    "\n",
    "            preprocesed_data = scraped_data.text.replace(\",\", \".\")\n",
    "\n",
    "            df = pd.read_html(preprocesed_data, header=0)[0]\n",
    "\n",
    "            base_name_list = df.columns.to_list()\n",
    "            rename_list = [\"Date\", \"Gold\", \"Silver\", \"Platinum\", \"Palladium\"]\n",
    "            columns = dict(zip(base_name_list, rename_list))\n",
    "            df.rename(columns=columns, inplace=True)\n",
    "\n",
    "            for i in df.columns.to_list():\n",
    "                try:\n",
    "                    df[i] = df[i].str.replace(\" \", \"\")\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "            df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True)\n",
    "            df.sort_values(by=\"Date\", inplace=True)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            df.sort_values(by=\"Date\", inplace=True)\n",
    "\n",
    "            db_frame = pd.read_excel(\"./data/centrobank/metalls.xlsx\", index_col=0)\n",
    "\n",
    "            result_df = pd.concat([df, db_frame]).drop_duplicates()\n",
    "\n",
    "            google_upload(result_df, f\"CB_Metalls_consolidate\")\n",
    "\n",
    "            with pd.ExcelWriter(\n",
    "                f\"./data/centrobank/metalls.xlsx\",\n",
    "                date_format=\"YYYY-MM-DD\",\n",
    "                datetime_format=\"YYYY-MM-DD\",\n",
    "            ) as writer:\n",
    "                result_df.to_excel(writer, sheet_name=f\"cb_metalls\")\n",
    "\n",
    "            print(result_df.tail())\n",
    "            break\n",
    "\n",
    "        except:\n",
    "            # print('Error occured!')\n",
    "            time.sleep(1)\n",
    "            number_of_tries += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#########################   NBK ################################\n",
    "################################################################\n",
    "\n",
    "async def nbk_tenge_async():\n",
    "    # Realy unrelieable source, mb it would be better off with using ms query inside the file\n",
    "    try:\n",
    "        year = date.today().year\n",
    "\n",
    "        upper_bound = f'01.01.2022'\n",
    "        lower_bound = f'31.12.{year}'\n",
    "\n",
    "        url = f'https://nationalbank.kz/ru/exchangerates/ezhednevnye-oficialnye-rynochnye-kursy-valyut\\\n",
    "            /report?rates%5B%5D=5&beginDate={upper_bound}&endDate={lower_bound}'\n",
    "\n",
    "        counter = 0\n",
    "\n",
    "        while counter <= 6:\n",
    "            try:\n",
    "                with time_limit(15):\n",
    "                    page = requests.get(url=url)\n",
    "                    break\n",
    "\n",
    "            except TimeoutException:\n",
    "                # print(\"NBK_tenge timed out! Another attempt\")\n",
    "                counter += 1\n",
    "\n",
    "        temp_df = pd.read_html(page.text)\n",
    "        df = temp_df[0]\n",
    "        df['Unnamed: 0'] = pd.to_datetime(df['Unnamed: 0'])  # , dayfirst=True)\n",
    "        df.rename(columns={'Unnamed: 0': 'date'}, inplace=True)\n",
    "\n",
    "        with pd.ExcelWriter(\n",
    "                '../parser_beta/data/nbk_tenge.xlsx') as writer:\n",
    "            df.to_excel(writer, sheet_name='tenge')\n",
    "\n",
    "        print('NBK_tenge parsing is DONE!')\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except ValueError:\n",
    "        print(\"Probably tech problem, check the source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "##########################  SHMET ##############################\n",
    "################################################################\n",
    "\n",
    "async def shmet_optimized_async():\n",
    "    url = \"https://en.shmet.com/api/rest/enweb/spot/getSpotPrice?code=baseMetal&size=10&currentLength=0\"\n",
    "    responce = requests.get(url)\n",
    "\n",
    "    day_df = pd.DataFrame(responce.json()[\"data\"])\n",
    "    day_df[\"date\"] = date.today()\n",
    "    cooper_row = day_df[day_df[\"name\"].str.contains(\"cu\", case=False)]\n",
    "\n",
    "    result = cooper_row[[\"date\", \"middle\", \"unit\"]]\n",
    "    result[\"date\"] = pd.to_datetime(result[\"date\"])\n",
    "    result = result.rename(columns={\"middle\": \"price\"})\n",
    "\n",
    "    hist_data = pd.read_excel(\"./data/shmet_historical.xlsx\", index_col=0)\n",
    "\n",
    "    new_df = pd.concat([result, hist_data], axis=0).reset_index(drop=True)\n",
    "    new_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/shmet_historical.xlsx\",\n",
    "        date_format=\"YYYY-MM-DD\",\n",
    "        datetime_format=\"YYYY-MM-DD\",\n",
    "    ) as writer:\n",
    "        new_df.to_excel(writer, sheet_name=\"SHMET\")\n",
    "\n",
    "    final = pd.read_excel(\"./data/shmet_historical.xlsx\", index_col=0)\n",
    "    final.drop_duplicates(inplace=True)\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/shmet_historical.xlsx\",\n",
    "        date_format=\"YYYY-MM-DD\",\n",
    "        datetime_format=\"YYYY-MM-DD\",\n",
    "    ) as writer:\n",
    "        final.to_excel(writer, sheet_name=\"SHMET\")\n",
    "\n",
    "    print(\"SHMET is done!!!\") \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "################## NEW_WESTMETALL###############################\n",
    "################################################################\n",
    "\n",
    "async def new_westmetall_async():\n",
    "    def get_data(metall, col_name):\n",
    "        url = f'https://www.westmetall.com/en/markdaten.php?action=table&field=LME_{metall}_cash'\n",
    "        response = requests.get(url=url)\n",
    "\n",
    "        df = pd.read_html(response.text)[0][:30]\n",
    "        data = df.iloc[:, :2]\n",
    "\n",
    "        data = data.query(\"date != 'date'\")\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data.iloc[:, 1] = pd.to_numeric(data.iloc[:, 1])\n",
    "        data.rename(columns={data.columns[1]: col_name}, inplace=True)\n",
    "\n",
    "        return data\n",
    "\n",
    "    al = get_data(metall='Al', col_name='aluminium')\n",
    "    cu = get_data(metall='Cu', col_name='copper')\n",
    "    pb = get_data(metall='Pb', col_name='lead')\n",
    "    ni = get_data(metall='Ni', col_name='nickel')\n",
    "    zn = get_data(metall='Zn', col_name='zink')\n",
    "    tn = get_data(metall='Sn', col_name='tin')\n",
    "\n",
    "    result = pd.merge(al, cu, on='date', how='left').merge(pb, on='date', how='left').merge(\n",
    "        ni, on='date', how='left').merge(zn, on='date', how='left').merge(tn, on='date', how='left')\n",
    "\n",
    "    old_data = pd.read_excel('./data/LME_westmetall_db.xlsx', index_col=0)\n",
    "\n",
    "    final_data = pd.concat([old_data, result], axis=0)\n",
    "\n",
    "    final_data.drop_duplicates(subset='date', inplace=True)\n",
    "\n",
    "    final_data.sort_values(by='date', inplace=True)\n",
    "\n",
    "    final_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/LME_westmetall_db.xlsx\",\n",
    "            date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "        final_data.to_excel(writer, sheet_name='LME_westmetall')\n",
    "    \n",
    "    print(\"WESTMETALL is done!!!\") \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WESTMETALL is done!!!\n",
      "LME_main is done!!!\n",
      "NBK_tenge parsing is DONE!\n",
      "SHMET is done!!!\n",
      "KITKO_main is done!!!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    await new_westmetall_async()\n",
    "    await lme_selenium_async()\n",
    "    await nbk_tenge_async()\n",
    "    await shmet_optimized_async()\n",
    "    await kitko_parser_async()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d40a6ae76423cf5fae73028028fb017d8630dd167584993d74bc99181da5a036"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
