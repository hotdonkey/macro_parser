{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main import block\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date, datetime\n",
    "\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Upload to google sheets\n",
    "import gspread\n",
    "#import df2gspread as d2g\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functions to maintain side manipulations #####\n",
    "\n",
    "\n",
    "# Timeout class for reattempting connection\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "\n",
    "# Function to convert date from json\n",
    "def date_format(date_raw):\n",
    "    timestamp = date_raw / 1000\n",
    "    date = datetime.fromtimestamp(timestamp)\n",
    "    formatted_date = date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return formatted_date\n",
    "\n",
    "\n",
    "def date_format_reverse():\n",
    "    date_now = date.today()\n",
    "    date_string = date_now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    date_raw = datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S\").timestamp()\n",
    "    date_raw = date_raw*1000\n",
    "    return int(date_raw)\n",
    "\n",
    "\n",
    "# Function for uploading dataframes into the google docs\n",
    "def google_upload(df, sheet_name):\n",
    "    # Params used to connect to google api\n",
    "    scope = [\n",
    "        'https://spreadsheets.google.com/feeds',\n",
    "        'https://www.googleapis.com/auth/drive']\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        'macro-parser-lme-c2f2972b48fc.json', scope)  # security token\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    # Key params for connection to particular document\n",
    "    spreadsheet_key = '1WhLiXRcdlkG7NCvHac9unC8ROt4lcbY7GxrOEdezZ9s'  # document id\n",
    "    wks_name = sheet_name  # sheet name that we use\n",
    "    d2g.upload(df, spreadsheet_key, wks_name, credentials=credentials)\n",
    "    # print(f'Uploading to {sheet_name} completed')\n",
    "\n",
    "\n",
    "# Session creation via proxy\n",
    "def get_session(url):\n",
    "\n",
    "    # Free proxy function\n",
    "    def get_free_proxies():\n",
    "        url = \"https://free-proxy-list.net/\"\n",
    "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "        raw_list = []\n",
    "        proxies = dict()\n",
    "        trs = soup.find('table').find_all('tr')  # main table\n",
    "\n",
    "        for i in trs[1:]:\n",
    "            raw_list.append(i.find_all('td'))  # list of raw data rows\n",
    "\n",
    "        for i in range(len(raw_list)):  # creating working proxy list\n",
    "            try:\n",
    "                if raw_list[i][6].text == 'yes':  # taking only https\n",
    "                    proxies[raw_list[i]\n",
    "                            [3].text] = f'{raw_list[i][0].text}:{raw_list[i][1].text}'\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "        adress = pd.Series(proxies)  # creating proxy series\n",
    "\n",
    "        return adress\n",
    "\n",
    "    # create session\n",
    "    session = requests.Session()\n",
    "\n",
    "    # random proxy\n",
    "    proxy = get_free_proxies()\n",
    "    counter = 0\n",
    "\n",
    "    while counter <= len(proxy):\n",
    "        try:\n",
    "            with time_limit(7):\n",
    "                random_proxy = proxy.sample().values[0]\n",
    "                session.proxies = {\"https\": random_proxy}\n",
    "                response = session.get(url)\n",
    "                break\n",
    "\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        except TimeoutException:\n",
    "            # print(\"NBK_tenge timed out! Another attempt\")\n",
    "            counter += 1\n",
    "            print(f'Attempt {counter+1}')\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "###### This is the main function block ######\n",
    "\n",
    "\n",
    "################################################################\n",
    "##############   LME ################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def lme_db_addition():\n",
    "\n",
    "    def get_day_info():\n",
    "\n",
    "        # URL API for every metall, place into the var\n",
    "        url_aluminium = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=1a0ef0b6-3ee6-4e44-a415-7a313d5bd771'\n",
    "        url_copper = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=762a3883-b0e1-4c18-b34b-fe97a1f2d3a5'\n",
    "        url_lead = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=bc443de6-0bdd-4464-8845-9504f528b0c6'\n",
    "        url_nikel = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=acadf037-c13f-42f2-b42a-cac9a8179940'\n",
    "        url_zink = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=c389e2b0-c4a3-46a0-96ca-69cacbe90ee4'\n",
    "\n",
    "        # List for iterations\n",
    "        req_list = {'aluminium': url_aluminium, 'copper': url_copper,\n",
    "                    'lead': url_lead, 'nickel': url_nikel, 'zink': url_zink}\n",
    "\n",
    "        # Empty dict for final row-stage\n",
    "        day_dict = {'date': [], 'aluminium': [], 'copper': [],\n",
    "                    'lead': [], 'nickel': [], 'zink': [], }\n",
    "\n",
    "        # Getting json from api's requests and taking the info (in our case OFFER price for a date)\n",
    "        for metal, url in req_list.items():\n",
    "            req = requests.get(url).json()\n",
    "            metal_dict = req['Rows'][0]\n",
    "            day_dict[metal] = float(metal_dict['Values'][1])\n",
    "            day_dict['date'] = metal_dict['BusinessDateTime']\n",
    "\n",
    "        # Transform dict from previous stage into the row\n",
    "        dict_ = dict(day_dict)  # Maybe can simplify this\n",
    "        day_row = pd.DataFrame([dict_])\n",
    "        day_row.date = pd.to_datetime(day_row.date)\n",
    "\n",
    "        return day_row  # This is our row for implimentation into the main base\n",
    "\n",
    "    # Opening main base, add a row, check for dupp, saving and closing\n",
    "    lme_db = pd.read_excel(\n",
    "        '../parser_beta/data/LME_db.xlsx', index_col=0)\n",
    "\n",
    "    try:\n",
    "        day_row = get_day_info()\n",
    "        lme_db = pd.concat([lme_db, day_row], axis=0, ignore_index=True)\n",
    "        lme_db.drop_duplicates(inplace=True)\n",
    "\n",
    "    except IndexError:\n",
    "        print('LME response is empty, please check the source.')\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/LME_db.xlsx\",\n",
    "        date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "        lme_db.to_excel(writer, sheet_name='LME_non_ferrous')\n",
    "    # print('LME parsing is DONE!')\n",
    "\n",
    "    google_upload(lme_db, 'LME_non_ferrous')\n",
    "\n",
    "    return lme_db\n",
    "\n",
    "\n",
    "################################################################\n",
    "##############   KITCO ################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def kitco_db():\n",
    "    # In KITCO parsing we're taking slightly different aproach, we need to reupload\n",
    "    # the table into the file because sometimes KITCO changing data backdating\n",
    "\n",
    "    #!!!Need to work out the implementation of previous year data!!!!\n",
    "    # year = int(date.today().year) - 2001\n",
    "\n",
    "    url = 'https://www.kitco.com/gold.londonfix.html'\n",
    "    # url_previous_year = f'https://www.kitco.com/londonfix/gold.londonfix{year}.html'\n",
    "\n",
    "    # response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}) # ordinal responce\n",
    "    response = get_session(url=url)  # response via proxy\n",
    "    # responce_prev = requests.get(url_previous_year, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    kitco_response = pd.read_html(response.text)\n",
    "    kitco_df = pd.read_excel(\n",
    "        '../parser_beta/data/kitko_db.xlsx', index_col=0)\n",
    "\n",
    "    # Get the raw table and drop unnesesary rows\n",
    "    kitco_day = kitco_response[1]\n",
    "    kitco_day.drop([1, 4, 6], axis=1, inplace=True)\n",
    "    kitco_day.columns = kitco_day.iloc[0]\n",
    "    kitco_day.drop([0, 1, 2], axis=0, inplace=True)\n",
    "\n",
    "    # Change tyoe of data within table\n",
    "    kitco_day['Date'] = pd.to_datetime(kitco_day['Date'])\n",
    "    kitco_day = kitco_day.replace({'-': np.nan})\n",
    "    kitco_day = kitco_day.sort_values(by=['Date'])\n",
    "    kitco_day = kitco_day.reset_index(drop=True)\n",
    "    kitco_day[['Gold', 'Silver', 'Platinum', 'Palladium']] = kitco_day[[\n",
    "        'Gold', 'Silver', 'Platinum', 'Palladium']].apply(pd.to_numeric)\n",
    "\n",
    "    kitco_day.drop_duplicates(inplace=True)\n",
    "\n",
    "    # And rewrite old table\n",
    "    kitco_day.to_excel(\n",
    "        '../parser_beta/data/kitko_db.xlsx', sheet_name='kitco_metall')\n",
    "    # print('KITCO parsing is DONE!')\n",
    "\n",
    "    google_upload(kitco_day, 'KITCO')\n",
    "\n",
    "    return kitco_day\n",
    "\n",
    "\n",
    "################################################################\n",
    "##############   CB ################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def cb_curr():\n",
    "    day = date.today()\n",
    "    today = day.strftime('%d/%m/%Y')\n",
    "\n",
    "    dict_of_currencies = {\n",
    "        'R01235': 'USD',\n",
    "        'R01239': 'EUR',\n",
    "        'R01010': 'Australian_Dollar',\n",
    "        'R01375': 'China_Yuan',\n",
    "        'R01035': 'British_Pound',\n",
    "        'R01335': 'Kazakhstan_Tenge',\n",
    "        'R01820': 'Japanese_Yen',\n",
    "        'R01775': 'Swiss_Franc'\n",
    "    }\n",
    "\n",
    "    list_of_currencies = [x for x in dict_of_currencies.keys()]\n",
    "\n",
    "    URL_list = []\n",
    "    for currency in list_of_currencies:\n",
    "        URL = f'http://www.cbr.ru/scripts/XML_dynamic.asp?date_req1={today}&date_req2={today}&VAL_NM_RQ={currency}'\n",
    "        URL_list.append(URL)\n",
    "\n",
    "    currency_df = pd.read_excel(\n",
    "        '../parser_beta/data/cb_curr.xlsx', index_col=0)\n",
    "\n",
    "    # This problem occurs in the beginning of the year so I was forced to catch a ValueError\n",
    "\n",
    "    try:\n",
    "        for url_element in URL_list:\n",
    "            response_df = pd.read_xml(url_element)\n",
    "            response_df['Date'] = pd.to_datetime(\n",
    "                response_df['Date'], dayfirst=True)\n",
    "            response_df['Value'] = response_df['Value'].apply(\n",
    "                lambda x: x.replace(',', '.'))\n",
    "            response_df['Value'] = response_df['Value'].apply(pd.to_numeric)\n",
    "            response_df = response_df.replace(dict_of_currencies)\n",
    "            currency_df = pd.concat(\n",
    "                [currency_df, response_df], axis=0, ignore_index=True)\n",
    "            currency_df.drop_duplicates(inplace=True)\n",
    "        # print('CentroBank_currency parsing is DONE!')\n",
    "\n",
    "        with pd.ExcelWriter(\n",
    "                \"../parser_beta/data/cb_curr.xlsx\") as writer:\n",
    "            currency_df.to_excel(writer, sheet_name='curr')\n",
    "\n",
    "        google_upload(currency_df, 'cb_curr')\n",
    "\n",
    "    except ValueError:\n",
    "        return 'Empty data set in CentroBank_currency. Should check the source.'\n",
    "\n",
    "    return currency_df\n",
    "\n",
    "\n",
    "def cb_metall():\n",
    "    day = date.today()\n",
    "    today = day.strftime('%d/%m/%Y')\n",
    "\n",
    "    metall_dict = {\n",
    "        1: 'gold',\n",
    "        2: 'silver',\n",
    "        3: 'platinum',\n",
    "        4: 'palladium'\n",
    "    }\n",
    "\n",
    "    URL = f'http://www.cbr.ru/scripts/xml_metall.asp?date_req1={today}&date_req2={today}'\n",
    "\n",
    "    metall_df = pd.read_excel(\n",
    "        '../parser_beta/data/cb_metall.xlsx', index_col=0)\n",
    "\n",
    "    # This problem occurs in the beginning of the year so I was forced to catch a ValueError\n",
    "    try:\n",
    "        response_df = pd.read_xml(URL)\n",
    "        response_df.drop(columns='Buy', axis=1, inplace=True)\n",
    "        response_df['Date'] = pd.to_datetime(\n",
    "            response_df['Date'], dayfirst=True)\n",
    "        response_df['Sell'] = response_df['Sell'].apply(\n",
    "            lambda x: x.replace(',', '.'))  # changing for future retyping to numeric\n",
    "        response_df['Sell'] = response_df['Sell'].apply(pd.to_numeric)\n",
    "        response_df = response_df.replace(metall_dict)\n",
    "\n",
    "        metall_df = pd.concat(\n",
    "            [metall_df, response_df],\n",
    "            axis=0,\n",
    "            ignore_index=True)\n",
    "\n",
    "        metall_df.drop_duplicates(inplace=True)\n",
    "        # print('CentroBank_metalls parsing is DONE!')\n",
    "\n",
    "        with pd.ExcelWriter(\n",
    "            '../parser_beta/data/cb_metall.xlsx',\n",
    "            date_format='YYYY-MM-DD',\n",
    "                datetime_format='YYYY-MM-DD') as writer:\n",
    "            metall_df.to_excel(writer, sheet_name='cb_metall')\n",
    "\n",
    "        google_upload(metall_df, 'cb_metall')\n",
    "\n",
    "    except ValueError:\n",
    "        return 'Empty data set in CentroBank_metalls . Should check the source.'\n",
    "\n",
    "    return metall_df\n",
    "\n",
    "\n",
    "################################################################\n",
    "##############   NBK ################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def nbk_tenge():\n",
    "    # Realy unrelieable source, mb it would be better off with using ms query inside the file\n",
    "    year = date.today().year\n",
    "\n",
    "    upper_bound = f'01.01.{year}'\n",
    "    lower_bound = f'31.12.{year}'\n",
    "\n",
    "    url = f'https://nationalbank.kz/ru/exchangerates/ezhednevnye-oficialnye-rynochnye-kursy-valyut\\\n",
    "        /report?rates%5B%5D=5&beginDate={upper_bound}&endDate={lower_bound}'\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    while counter <= 6:\n",
    "        try:\n",
    "            with time_limit(15):\n",
    "                page = requests.get(url=url)\n",
    "                break\n",
    "\n",
    "        except TimeoutException:\n",
    "            # print(\"NBK_tenge timed out! Another attempt\")\n",
    "            counter += 1\n",
    "\n",
    "    temp_df = pd.read_html(page.text)\n",
    "    df = temp_df[0]\n",
    "    df['Unnamed: 0'] = pd.to_datetime(df['Unnamed: 0'])#, dayfirst=True)\n",
    "    df.rename(columns={'Unnamed: 0': 'date'}, inplace=True)\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "            '../parser_beta/data/nbk_tenge.xlsx') as writer:\n",
    "        df.to_excel(writer, sheet_name='tenge')\n",
    "\n",
    "    # print('NBK_tenge parsing is DONE!')\n",
    "\n",
    "    google_upload(df, 'nbk_tenge')\n",
    "\n",
    "    return df\n",
    "\n",
    "################################################################\n",
    "##############   SHMET ################\n",
    "################################################################\n",
    "\n",
    "def shmet_parser():\n",
    "    def shmet_day():\n",
    "        day = date.today()\n",
    "        today = date_format_reverse()\n",
    "        # =1690186841910'\n",
    "        url = f'https://en.shmet.com/api/rest/enweb/spot/getChartPrices?scId=811&startDate={day}&endDate={day}&_{today}'\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:  # Проверка успешности запроса\n",
    "            data = response.json()  # Получение JSON-данных из ответа\n",
    "\n",
    "        with open(\"./data/shmet.json\", \"w\") as file:  # Открытие файла для записи\n",
    "            json.dump(data, file)  # Запись JSON-данных в файл\n",
    "\n",
    "        temp = pd.read_json('./data/shmet.json')['data']['spotPrices']\n",
    "        data = pd.DataFrame(temp)\n",
    "\n",
    "        data['dateTime'] = data['dateTime'].apply(date_format)\n",
    "        data.rename({'dateTime': 'date'}, axis=1, inplace=True)\n",
    "        data['date'] = pd.to_datetime(data['date']).dt.date\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "        return data\n",
    "\n",
    "    df = pd.read_excel('./data/shmet_historical.xlsx', index_col=0)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    try:\n",
    "        day_row = shmet_day()\n",
    "        shmet_db = pd.concat([day_row, df], axis=0)\n",
    "\n",
    "        shmet_db = shmet_db.reset_index(drop=True)\n",
    "        shmet_db.drop_duplicates(inplace=True)\n",
    "\n",
    "        with pd.ExcelWriter(\n",
    "                \"../parser_beta/data/shmet_historical.xlsx\",\n",
    "                date_format=\"YYYY-MM-DD\",\n",
    "                datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "            shmet_db.to_excel(writer, sheet_name='SHMET')\n",
    "\n",
    "    except (IndexError, KeyError) as e:\n",
    "        print('Shmet day is empty, please check the source.')\n",
    "        return None\n",
    "\n",
    "    final = pd.read_excel('./data/shmet_historical.xlsx', index_col=0)\n",
    "    final.drop_duplicates(inplace=True)\n",
    "\n",
    "    google_upload(final, 'SHMET')\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/shmet_historical.xlsx\",\n",
    "            date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "        final.to_excel(writer, sheet_name='SHMET')\n",
    "\n",
    "    return final\n",
    "\n",
    "################################################################\n",
    "##############   WESTMETAL (dupplicate for LME) ################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def westmetall():\n",
    "    def aluminium():\n",
    "        url = 'https://www.westmetall.com/en/markdaten.php?action=table&field=LME_Al_cash'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_elements = page.find_all('td')\n",
    "\n",
    "        aluminium_date = pd.to_datetime(page_elements[0].text)\n",
    "        aluminium_settlment = page_elements[1].text\n",
    "\n",
    "        aluminium = pd.DataFrame(\n",
    "            data={'date': aluminium_date, 'aluminium': aluminium_settlment}, index=[0])\n",
    "        # aluminium['aluminium'] = pd.to_numeric(aluminium['aluminium'])\n",
    "\n",
    "        return aluminium\n",
    "\n",
    "    def copper():\n",
    "        url = 'https://www.westmetall.com/en/markdaten.php?action=table&field=LME_Cu_cash'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_elements = page.find_all('td')\n",
    "\n",
    "        copper_date = pd.to_datetime(page_elements[0].text)\n",
    "        copper_settlment = page_elements[1].text\n",
    "\n",
    "        copper = pd.DataFrame(\n",
    "            data={'date': copper_date, 'copper': copper_settlment}, index=[0])\n",
    "        # copper['copper'] = pd.to_numeric(copper['copper'])\n",
    "\n",
    "        return copper\n",
    "\n",
    "    def lead():\n",
    "        url = 'https://www.westmetall.com/en/markdaten.php?action=table&field=LME_Pb_cash'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_elements = page.find_all('td')\n",
    "\n",
    "        lead_date = pd.to_datetime(page_elements[0].text)\n",
    "        lead_settlment = page_elements[1].text\n",
    "\n",
    "        lead = pd.DataFrame(\n",
    "            data={'date': lead_date, 'lead': lead_settlment}, index=[0])\n",
    "        # lead['lead'] = pd.to_numeric(lead['lead'])\n",
    "\n",
    "        return lead\n",
    "\n",
    "    def nickel():\n",
    "        url = 'https://www.westmetall.com/en/markdaten.php?action=table&field=LME_Ni_cash'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_elements = page.find_all('td')\n",
    "\n",
    "        nickel_date = pd.to_datetime(page_elements[0].text)\n",
    "        nickel_settlment = page_elements[1].text\n",
    "\n",
    "        nickel = pd.DataFrame(\n",
    "            data={'date': nickel_date, 'nickel': nickel_settlment}, index=[0])\n",
    "        # nickel['nickel'] = pd.to_numeric(nickel['nickel'])\n",
    "\n",
    "        return nickel\n",
    "\n",
    "    def zink():\n",
    "        url = 'https://www.westmetall.com/en/markdaten.php?action=table&field=LME_Zn_cash'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_elements = page.find_all('td')\n",
    "\n",
    "        zink_date = pd.to_datetime(page_elements[0].text)\n",
    "        zink_settlment = page_elements[1].text\n",
    "\n",
    "        zink = pd.DataFrame(\n",
    "            data={'date': zink_date, 'zink': zink_settlment}, index=[0])\n",
    "        # zink['zink'] = pd.to_numeric(zink['zink'])\n",
    "\n",
    "        return zink\n",
    "\n",
    "    al_data = aluminium()\n",
    "    cu_data = copper()\n",
    "    ld_data = lead()\n",
    "    nk_data = nickel()\n",
    "    zk_data = zink()\n",
    "\n",
    "    result = pd.merge(pd.merge(pd.merge(pd.merge(al_data, cu_data, on='date', suffixes=['_al', '_cu']),\n",
    "                                        ld_data, on='date'), nk_data, on='date'), zk_data, on='date')\n",
    "\n",
    "    result['aluminium'] = result['aluminium'].str.replace(',', '')\n",
    "    result['copper'] = result['copper'].str.replace(',', '')\n",
    "    result['lead'] = result['lead'].str.replace(',', '')\n",
    "    result['nickel'] = result['nickel'].str.replace(',', '')\n",
    "    result['zink'] = result['zink'].str.replace(',', '')\n",
    "\n",
    "    df = pd.read_excel('./data/LME_westmetall_db.xlsx', index_col=0)\n",
    "    df = pd.concat([df, result], axis=0)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/LME_westmetall_db.xlsx\",\n",
    "            date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "        df.to_excel(writer, sheet_name='LME_westmetall')\n",
    "\n",
    "    df = pd.read_excel('./data/LME_westmetall_db.xlsx', index_col=0)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/LME_westmetall_db.xlsx\",\n",
    "            date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "        df.to_excel(writer, sheet_name='LME_westmetall')\n",
    "\n",
    "    google_upload(df, 'LME_westmetall')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "##### Run!!! #####\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#    functions = {\n",
    "#        'LME': lme_db_addition, 'CB_CURR': cb_curr,\n",
    "#        'CB_METAL': cb_metall, 'NBK': nbk_tenge, 'KITCO': kitco_db\n",
    "#    }\n",
    "\n",
    "#    for number, func in tqdm(functions.items()):\n",
    "#        try:\n",
    "#            func()\n",
    "\n",
    "#        except ValueError:\n",
    "#            print(f'Supposed problems on {number} side')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lme_db_addition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>aluminium</th>\n",
       "      <th>copper</th>\n",
       "      <th>lead</th>\n",
       "      <th>nickel</th>\n",
       "      <th>zink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>2815.5</td>\n",
       "      <td>9660.0</td>\n",
       "      <td>2327.0</td>\n",
       "      <td>20730.0</td>\n",
       "      <td>3602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>2866.0</td>\n",
       "      <td>9778.0</td>\n",
       "      <td>2343.0</td>\n",
       "      <td>20900.0</td>\n",
       "      <td>3660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>2912.5</td>\n",
       "      <td>9565.0</td>\n",
       "      <td>2291.0</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>3590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2919.5</td>\n",
       "      <td>9615.0</td>\n",
       "      <td>2342.0</td>\n",
       "      <td>20725.0</td>\n",
       "      <td>3602.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-10</td>\n",
       "      <td>2923.0</td>\n",
       "      <td>9665.0</td>\n",
       "      <td>2303.0</td>\n",
       "      <td>21045.0</td>\n",
       "      <td>3576.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>2068.5</td>\n",
       "      <td>8239.5</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>20050.0</td>\n",
       "      <td>2270.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>2023-08-22</td>\n",
       "      <td>2121.5</td>\n",
       "      <td>8342.0</td>\n",
       "      <td>2195.0</td>\n",
       "      <td>20215.0</td>\n",
       "      <td>2336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>2129.0</td>\n",
       "      <td>8377.5</td>\n",
       "      <td>2190.0</td>\n",
       "      <td>20525.0</td>\n",
       "      <td>2328.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>2023-08-24</td>\n",
       "      <td>2119.5</td>\n",
       "      <td>8358.5</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>20400.0</td>\n",
       "      <td>2363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>2023-08-25</td>\n",
       "      <td>2124.0</td>\n",
       "      <td>8381.5</td>\n",
       "      <td>2178.0</td>\n",
       "      <td>20900.0</td>\n",
       "      <td>2375.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  aluminium  copper    lead   nickel    zink\n",
       "0   2022-01-04     2815.5  9660.0  2327.0  20730.0  3602.0\n",
       "1   2022-01-05     2866.0  9778.0  2343.0  20900.0  3660.0\n",
       "2   2022-01-06     2912.5  9565.0  2291.0  20480.0  3590.0\n",
       "3   2022-01-07     2919.5  9615.0  2342.0  20725.0  3602.5\n",
       "4   2022-01-10     2923.0  9665.0  2303.0  21045.0  3576.5\n",
       "..         ...        ...     ...     ...      ...     ...\n",
       "410 2023-08-21     2068.5  8239.5  2168.0  20050.0  2270.5\n",
       "411 2023-08-22     2121.5  8342.0  2195.0  20215.0  2336.0\n",
       "412 2023-08-23     2129.0  8377.5  2190.0  20525.0  2328.5\n",
       "413 2023-08-24     2119.5  8358.5  2214.0  20400.0  2363.0\n",
       "414 2023-08-25     2124.0  8381.5  2178.0  20900.0  2375.5\n",
       "\n",
       "[415 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "westmetall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Empty data set in CentroBank_currency. Should check the source.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_curr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Empty data set in CentroBank_metalls . Should check the source.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_metall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Числовое значение</th>\n",
       "      <th>ДОЛЛАР США</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>462.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>462.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>462.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>462.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>465.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>1</td>\n",
       "      <td>453.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2023-08-24</td>\n",
       "      <td>1</td>\n",
       "      <td>458.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2023-08-25</td>\n",
       "      <td>1</td>\n",
       "      <td>462.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>1</td>\n",
       "      <td>460.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2023-08-27</td>\n",
       "      <td>1</td>\n",
       "      <td>460.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  Числовое значение  ДОЛЛАР США\n",
       "0   2023-01-01                  1      462.65\n",
       "1   2023-01-02                  1      462.65\n",
       "2   2023-01-03                  1      462.65\n",
       "3   2023-01-04                  1      462.65\n",
       "4   2023-01-05                  1      465.39\n",
       "..         ...                ...         ...\n",
       "234 2023-08-23                  1      453.13\n",
       "235 2023-08-24                  1      458.13\n",
       "236 2023-08-25                  1      462.04\n",
       "237 2023-08-26                  1      460.17\n",
       "238 2023-08-27                  1      460.17\n",
       "\n",
       "[239 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbk_tenge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Silver</th>\n",
       "      <th>Platinum</th>\n",
       "      <th>Palladium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>1843.25</td>\n",
       "      <td>24.295</td>\n",
       "      <td>1082.0</td>\n",
       "      <td>1795.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>1857.30</td>\n",
       "      <td>24.290</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>1834.00</td>\n",
       "      <td>23.410</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>1783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>1852.20</td>\n",
       "      <td>23.455</td>\n",
       "      <td>1073.0</td>\n",
       "      <td>1784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>1878.85</td>\n",
       "      <td>23.850</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>1793.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2023-08-22</td>\n",
       "      <td>1892.75</td>\n",
       "      <td>23.390</td>\n",
       "      <td>928.0</td>\n",
       "      <td>1289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>1916.65</td>\n",
       "      <td>23.750</td>\n",
       "      <td>929.0</td>\n",
       "      <td>1282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2023-08-24</td>\n",
       "      <td>1917.05</td>\n",
       "      <td>24.185</td>\n",
       "      <td>933.0</td>\n",
       "      <td>1244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2023-08-25</td>\n",
       "      <td>1.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>949.0</td>\n",
       "      <td>1233.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0         Date     Gold  Silver  Platinum  Palladium\n",
       "0   2023-01-03  1843.25  24.295    1082.0     1795.0\n",
       "1   2023-01-04  1857.30  24.290    1080.0     1736.0\n",
       "2   2023-01-05  1834.00  23.410    1062.0     1783.0\n",
       "3   2023-01-06  1852.20  23.455    1073.0     1784.0\n",
       "4   2023-01-09  1878.85  23.850    1092.0     1793.0\n",
       "..         ...      ...     ...       ...        ...\n",
       "160 2023-08-22  1892.75  23.390     928.0     1289.0\n",
       "161 2023-08-23  1916.65  23.750     929.0     1282.0\n",
       "162 2023-08-24  1917.05  24.185     933.0     1244.0\n",
       "163 2023-08-25     1.36     NaN     949.0     1233.0\n",
       "164        NaT      NaN     NaN       NaN        NaN\n",
       "\n",
       "[165 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kitco_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shmet day is empty, please check the source.\n"
     ]
    }
   ],
   "source": [
    "shmet_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d40a6ae76423cf5fae73028028fb017d8630dd167584993d74bc99181da5a036"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
