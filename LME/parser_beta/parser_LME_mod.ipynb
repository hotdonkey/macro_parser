{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main import block\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date, datetime\n",
    "\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Upload to google sheets\n",
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functions to maintain side manipulations #####\n",
    "\n",
    "\n",
    "# Timeout class for reattempting connection\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "\n",
    "# Function to convert date from json\n",
    "def date_format(date_raw):\n",
    "    timestamp = date_raw / 1000\n",
    "    date = datetime.fromtimestamp(timestamp)\n",
    "    formatted_date = date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return formatted_date\n",
    "\n",
    "\n",
    "def date_format_reverse():\n",
    "    date_now = date.today()\n",
    "    date_string = date_now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    date_raw = datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S\").timestamp()\n",
    "    date_raw = date_raw*1000\n",
    "    return int(date_raw)\n",
    "\n",
    "\n",
    "# Function for uploading dataframes into the google docs\n",
    "def google_upload(df, sheet_name):\n",
    "    # Params used to connect to google api\n",
    "    scope = [\n",
    "        'https://spreadsheets.google.com/feeds',\n",
    "        'https://www.googleapis.com/auth/drive']\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        'macro-parser-lme-c2f2972b48fc.json', scope)  # security token\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    # Key params for connection to particular document\n",
    "    spreadsheet_key = '1WhLiXRcdlkG7NCvHac9unC8ROt4lcbY7GxrOEdezZ9s'  # document id\n",
    "    wks_name = sheet_name  # sheet name that we use\n",
    "    d2g.upload(df, spreadsheet_key, wks_name, credentials=credentials)\n",
    "    # print(f'Uploading to {sheet_name} completed')\n",
    "\n",
    "\n",
    "# Session creation via proxy\n",
    "def get_session(url):\n",
    "\n",
    "    # Free proxy function\n",
    "    def get_free_proxies():\n",
    "        url = \"https://free-proxy-list.net/\"\n",
    "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "        raw_list = []\n",
    "        proxies = dict()\n",
    "        trs = soup.find('table').find_all('tr')  # main table\n",
    "\n",
    "        for i in trs[1:]:\n",
    "            raw_list.append(i.find_all('td'))  # list of raw data rows\n",
    "\n",
    "        for i in range(len(raw_list)):  # creating working proxy list\n",
    "            try:\n",
    "                if raw_list[i][6].text == 'yes':  # taking only https\n",
    "                    proxies[raw_list[i]\n",
    "                            [3].text] = f'{raw_list[i][0].text}:{raw_list[i][1].text}'\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "        adress = pd.Series(proxies)  # creating proxy series\n",
    "\n",
    "        return adress\n",
    "\n",
    "    # create session\n",
    "    session = requests.Session()\n",
    "\n",
    "    # random proxy\n",
    "    proxy = get_free_proxies()\n",
    "    counter = 0\n",
    "\n",
    "    while counter <= len(proxy):\n",
    "        try:\n",
    "            with time_limit(7):\n",
    "                random_proxy = proxy.sample().values[0]\n",
    "                session.proxies = {\"https\": random_proxy}\n",
    "                response = session.get(url)\n",
    "                break\n",
    "\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        except TimeoutException:\n",
    "            # print(\"NBK_tenge timed out! Another attempt\")\n",
    "            counter += 1\n",
    "            print(f'Attempt {counter+1}')\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "###### This is the main function block ######\n",
    "\n",
    "\n",
    "################################################################\n",
    "##############   LME ################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def lme_db_addition():\n",
    "\n",
    "    def get_day_info():\n",
    "\n",
    "        # URL API for every metall, place into the var\n",
    "        url_aluminium = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=1a0ef0b6-3ee6-4e44-a415-7a313d5bd771'\n",
    "        url_copper = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=762a3883-b0e1-4c18-b34b-fe97a1f2d3a5'\n",
    "        url_lead = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=bc443de6-0bdd-4464-8845-9504f528b0c6'\n",
    "        url_nikel = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=acadf037-c13f-42f2-b42a-cac9a8179940'\n",
    "        url_zink = 'https://www.lme.com/api/trading-data/day-delayed?datasourceId=c389e2b0-c4a3-46a0-96ca-69cacbe90ee4'\n",
    "\n",
    "        # List for iterations\n",
    "        req_list = {'aluminium': url_aluminium, 'copper': url_copper,\n",
    "                    'lead': url_lead, 'nickel': url_nikel, 'zink': url_zink}\n",
    "\n",
    "        # Empty dict for final row-stage\n",
    "        day_dict = {'date': [], 'aluminium': [], 'copper': [],\n",
    "                    'lead': [], 'nickel': [], 'zink': [], }\n",
    "\n",
    "        # Getting json from api's requests and taking the info (in our case OFFER price for a date)\n",
    "        for metal, url in req_list.items():\n",
    "            req = requests.get(url).json()\n",
    "            metal_dict = req['Rows'][0]\n",
    "            day_dict[metal] = float(metal_dict['Values'][1])\n",
    "            day_dict['date'] = metal_dict['BusinessDateTime']\n",
    "\n",
    "        # Transform dict from previous stage into the row\n",
    "        dict_ = dict(day_dict)  # Maybe can simplify this\n",
    "        day_row = pd.DataFrame([dict_])\n",
    "        day_row.date = pd.to_datetime(day_row.date)\n",
    "\n",
    "        return day_row  # This is our row for implimentation into the main base\n",
    "\n",
    "    # Opening main base, add a row, check for dupp, saving and closing\n",
    "    lme_db = pd.read_excel(\n",
    "        '../parser_beta/data/LME_db.xlsx', index_col=0)\n",
    "\n",
    "    try:\n",
    "        day_row = get_day_info()\n",
    "        lme_db = pd.concat([lme_db, day_row], axis=0, ignore_index=True)\n",
    "        lme_db.drop_duplicates(inplace=True)\n",
    "\n",
    "    except IndexError:\n",
    "        print('LME response is empty, please check the source.')\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/LME_db.xlsx\",\n",
    "        date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "        lme_db.to_excel(writer, sheet_name='LME_non_ferrous')\n",
    "    # print('LME parsing is DONE!')\n",
    "\n",
    "    google_upload(lme_db, 'LME_non_ferrous')\n",
    "\n",
    "    return lme_db\n",
    "\n",
    "\n",
    "################################################################\n",
    "##############   KITCO ################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def kitco_db():\n",
    "    # In KITCO parsing we're taking slightly different aproach, we need to reupload\n",
    "    # the table into the file because sometimes KITCO changing data backdating\n",
    "\n",
    "    #!!!Need to work out the implementation of previous year data!!!!\n",
    "    # year = int(date.today().year) - 2001\n",
    "\n",
    "    url = 'https://www.kitco.com/gold.londonfix.html'\n",
    "    # url_previous_year = f'https://www.kitco.com/londonfix/gold.londonfix{year}.html'\n",
    "\n",
    "    # response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}) # ordinal responce\n",
    "    response = get_session(url=url)  # response via proxy\n",
    "    # responce_prev = requests.get(url_previous_year, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    kitco_response = pd.read_html(response.text)\n",
    "    kitco_df = pd.read_excel(\n",
    "        '../parser_beta/data/kitko_db.xlsx', index_col=0)\n",
    "\n",
    "    # Get the raw table and drop unnesesary rows\n",
    "    kitco_day = kitco_response[1]\n",
    "    kitco_day.drop([1, 4, 6], axis=1, inplace=True)\n",
    "    kitco_day.columns = kitco_day.iloc[0]\n",
    "    kitco_day.drop([0, 1, 2], axis=0, inplace=True)\n",
    "\n",
    "    # Change tyoe of data within table\n",
    "    kitco_day['Date'] = pd.to_datetime(kitco_day['Date'])\n",
    "    kitco_day = kitco_day.replace({'-': np.nan})\n",
    "    kitco_day = kitco_day.sort_values(by=['Date'])\n",
    "    kitco_day = kitco_day.reset_index(drop=True)\n",
    "    kitco_day[['Gold', 'Silver', 'Platinum', 'Palladium']] = kitco_day[[\n",
    "        'Gold', 'Silver', 'Platinum', 'Palladium']].apply(pd.to_numeric)\n",
    "\n",
    "    kitco_day.drop_duplicates(inplace=True)\n",
    "\n",
    "    # And rewrite old table\n",
    "    kitco_day.to_excel(\n",
    "        '../parser_beta/data/kitko_db.xlsx', sheet_name='kitco_metall')\n",
    "    # print('KITCO parsing is DONE!')\n",
    "\n",
    "    google_upload(kitco_day, 'KITCO')\n",
    "\n",
    "    return kitco_day\n",
    "\n",
    "\n",
    "################################################################\n",
    "##############   CB ################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def cb_curr():\n",
    "    day = date.today()\n",
    "    today = day.strftime('%d/%m/%Y')\n",
    "\n",
    "    dict_of_currencies = {\n",
    "        'R01235': 'USD',\n",
    "        'R01239': 'EUR',\n",
    "        'R01010': 'Australian_Dollar',\n",
    "        'R01375': 'China_Yuan',\n",
    "        'R01035': 'British_Pound',\n",
    "        'R01335': 'Kazakhstan_Tenge',\n",
    "        'R01820': 'Japanese_Yen',\n",
    "        'R01775': 'Swiss_Franc'\n",
    "    }\n",
    "\n",
    "    list_of_currencies = [x for x in dict_of_currencies.keys()]\n",
    "\n",
    "    URL_list = []\n",
    "    for currency in list_of_currencies:\n",
    "        URL = f'http://www.cbr.ru/scripts/XML_dynamic.asp?date_req1={today}&date_req2={today}&VAL_NM_RQ={currency}'\n",
    "        URL_list.append(URL)\n",
    "\n",
    "    currency_df = pd.read_excel(\n",
    "        '../parser_beta/data/cb_curr.xlsx', index_col=0)\n",
    "\n",
    "    # This problem occurs in the beginning of the year so I was forced to catch a ValueError\n",
    "\n",
    "    try:\n",
    "        for url_element in URL_list:\n",
    "            response_df = pd.read_xml(url_element)\n",
    "            response_df['Date'] = pd.to_datetime(\n",
    "                response_df['Date'], dayfirst=True)\n",
    "            response_df['Value'] = response_df['Value'].apply(\n",
    "                lambda x: x.replace(',', '.'))\n",
    "            response_df['Value'] = response_df['Value'].apply(pd.to_numeric)\n",
    "            response_df = response_df.replace(dict_of_currencies)\n",
    "            currency_df = pd.concat(\n",
    "                [currency_df, response_df], axis=0, ignore_index=True)\n",
    "            currency_df.drop_duplicates(inplace=True)\n",
    "        # print('CentroBank_currency parsing is DONE!')\n",
    "\n",
    "        with pd.ExcelWriter(\n",
    "                \"../parser_beta/data/cb_curr.xlsx\") as writer:\n",
    "            currency_df.to_excel(writer, sheet_name='curr')\n",
    "\n",
    "        google_upload(currency_df, 'cb_curr')\n",
    "\n",
    "    except ValueError:\n",
    "        return 'Empty data set in CentroBank_currency. Should check the source.'\n",
    "\n",
    "    return currency_df\n",
    "\n",
    "\n",
    "def cb_metall():\n",
    "    day = date.today()\n",
    "    today = day.strftime('%d/%m/%Y')\n",
    "\n",
    "    metall_dict = {\n",
    "        1: 'gold',\n",
    "        2: 'silver',\n",
    "        3: 'platinum',\n",
    "        4: 'palladium'\n",
    "    }\n",
    "\n",
    "    URL = f'http://www.cbr.ru/scripts/xml_metall.asp?date_req1={today}&date_req2={today}'\n",
    "\n",
    "    metall_df = pd.read_excel(\n",
    "        '../parser_beta/data/cb_metall.xlsx', index_col=0)\n",
    "\n",
    "    # This problem occurs in the beginning of the year so I was forced to catch a ValueError\n",
    "    try:\n",
    "        response_df = pd.read_xml(URL)\n",
    "        response_df.drop(columns='Buy', axis=1, inplace=True)\n",
    "        response_df['Date'] = pd.to_datetime(\n",
    "            response_df['Date'], dayfirst=True)\n",
    "        response_df['Sell'] = response_df['Sell'].apply(\n",
    "            lambda x: x.replace(',', '.'))  # changing for future retyping to numeric\n",
    "        response_df['Sell'] = response_df['Sell'].apply(pd.to_numeric)\n",
    "        response_df = response_df.replace(metall_dict)\n",
    "\n",
    "        metall_df = pd.concat(\n",
    "            [metall_df, response_df],\n",
    "            axis=0,\n",
    "            ignore_index=True)\n",
    "\n",
    "        metall_df.drop_duplicates(inplace=True)\n",
    "        # print('CentroBank_metalls parsing is DONE!')\n",
    "\n",
    "        with pd.ExcelWriter(\n",
    "            '../parser_beta/data/cb_metall.xlsx',\n",
    "            date_format='YYYY-MM-DD',\n",
    "                datetime_format='YYYY-MM-DD') as writer:\n",
    "            metall_df.to_excel(writer, sheet_name='cb_metall')\n",
    "\n",
    "        google_upload(metall_df, 'cb_metall')\n",
    "\n",
    "    except ValueError:\n",
    "        return 'Empty data set in CentroBank_metalls . Should check the source.'\n",
    "\n",
    "    return metall_df\n",
    "\n",
    "\n",
    "################################################################\n",
    "##############   NBK ################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def nbk_tenge():\n",
    "    # Realy unrelieable source, mb it would be better off with using ms query inside the file\n",
    "    year = date.today().year\n",
    "\n",
    "    upper_bound = f'01.01.{year}'\n",
    "    lower_bound = f'31.12.{year}'\n",
    "\n",
    "    url = f'https://nationalbank.kz/ru/exchangerates/ezhednevnye-oficialnye-rynochnye-kursy-valyut\\\n",
    "        /report?rates%5B%5D=5&beginDate={upper_bound}&endDate={lower_bound}'\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    while counter <= 6:\n",
    "        try:\n",
    "            with time_limit(15):\n",
    "                page = requests.get(url=url)\n",
    "                break\n",
    "\n",
    "        except TimeoutException:\n",
    "            # print(\"NBK_tenge timed out! Another attempt\")\n",
    "            counter += 1\n",
    "\n",
    "    temp_df = pd.read_html(page.text)\n",
    "    df = temp_df[0]\n",
    "    df['Unnamed: 0'] = pd.to_datetime(df['Unnamed: 0'], dayfirst=True)\n",
    "    df.rename(columns={'Unnamed: 0': 'date'}, inplace=True)\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "            '../parser_beta/data/nbk_tenge.xlsx') as writer:\n",
    "        df.to_excel(writer, sheet_name='tenge')\n",
    "\n",
    "    # print('NBK_tenge parsing is DONE!')\n",
    "\n",
    "    google_upload(df, 'nbk_tenge')\n",
    "\n",
    "    return df\n",
    "\n",
    "################################################################\n",
    "##############   SHMET ################\n",
    "################################################################\n",
    "\n",
    "def shmet_parser():\n",
    "    def shmet_day():\n",
    "        day = date.today()\n",
    "        today = date_format_reverse()\n",
    "        # =1690186841910'\n",
    "        url = f'https://en.shmet.com/api/rest/enweb/spot/getChartPrices?scId=811&startDate={day}&endDate={day}&_{today}'\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:  # Проверка успешности запроса\n",
    "            data = response.json()  # Получение JSON-данных из ответа\n",
    "\n",
    "        with open(\"./data/shmet.json\", \"w\") as file:  # Открытие файла для записи\n",
    "            json.dump(data, file)  # Запись JSON-данных в файл\n",
    "\n",
    "        temp = pd.read_json('./data/shmet.json')['data']['spotPrices']\n",
    "        data = pd.DataFrame(temp)\n",
    "\n",
    "        data['dateTime'] = data['dateTime'].apply(date_format)\n",
    "        data.rename({'dateTime': 'date'}, axis=1, inplace=True)\n",
    "        data['date'] = pd.to_datetime(data['date']).dt.date\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "        return data\n",
    "\n",
    "    df = pd.read_excel('./data/shmet_historical.xlsx', index_col=0)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    try:\n",
    "        day_row = shmet_day()\n",
    "        shmet_db = pd.concat([day_row, df], axis=0)\n",
    "\n",
    "        shmet_db = shmet_db.reset_index(drop=True)\n",
    "        shmet_db.drop_duplicates(inplace=True)\n",
    "\n",
    "        with pd.ExcelWriter(\n",
    "                \"../parser_beta/data/shmet_historical.xlsx\",\n",
    "                date_format=\"YYYY-MM-DD\",\n",
    "                datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "            shmet_db.to_excel(writer, sheet_name='SHMET')\n",
    "\n",
    "    except (IndexError, KeyError) as e:\n",
    "        print('Shmet day is empty, please check the source.')\n",
    "        return None\n",
    "\n",
    "    final = pd.read_excel('./data/shmet_historical.xlsx', index_col=0)\n",
    "    final.drop_duplicates(inplace=True)\n",
    "\n",
    "    google_upload(final, 'SHMET')\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/shmet_historical.xlsx\",\n",
    "            date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "        final.to_excel(writer, sheet_name='SHMET')\n",
    "\n",
    "    return final\n",
    "\n",
    "################################################################\n",
    "##############   WESTMETAL (dupplicate for LME) ################\n",
    "################################################################\n",
    "\n",
    "\n",
    "def westmetall():\n",
    "    def aluminium():\n",
    "        url = 'https://www.westmetall.com/en/markdaten.php?action=table&field=LME_Al_cash'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_elements = page.find_all('td')\n",
    "\n",
    "        aluminium_date = pd.to_datetime(page_elements[0].text)\n",
    "        aluminium_settlment = page_elements[1].text\n",
    "\n",
    "        aluminium = pd.DataFrame(\n",
    "            data={'date': aluminium_date, 'aluminium': aluminium_settlment}, index=[0])\n",
    "        # aluminium['aluminium'] = pd.to_numeric(aluminium['aluminium'])\n",
    "\n",
    "        return aluminium\n",
    "\n",
    "    def copper():\n",
    "        url = 'https://www.westmetall.com/en/markdaten.php?action=table&field=LME_Cu_cash'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_elements = page.find_all('td')\n",
    "\n",
    "        copper_date = pd.to_datetime(page_elements[0].text)\n",
    "        copper_settlment = page_elements[1].text\n",
    "\n",
    "        copper = pd.DataFrame(\n",
    "            data={'date': copper_date, 'copper': copper_settlment}, index=[0])\n",
    "        # copper['copper'] = pd.to_numeric(copper['copper'])\n",
    "\n",
    "        return copper\n",
    "\n",
    "    def lead():\n",
    "        url = 'https://www.westmetall.com/en/markdaten.php?action=table&field=LME_Pb_cash'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_elements = page.find_all('td')\n",
    "\n",
    "        lead_date = pd.to_datetime(page_elements[0].text)\n",
    "        lead_settlment = page_elements[1].text\n",
    "\n",
    "        lead = pd.DataFrame(\n",
    "            data={'date': lead_date, 'lead': lead_settlment}, index=[0])\n",
    "        # lead['lead'] = pd.to_numeric(lead['lead'])\n",
    "\n",
    "        return lead\n",
    "\n",
    "    def nickel():\n",
    "        url = 'https://www.westmetall.com/en/markdaten.php?action=table&field=LME_Ni_cash'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_elements = page.find_all('td')\n",
    "\n",
    "        nickel_date = pd.to_datetime(page_elements[0].text)\n",
    "        nickel_settlment = page_elements[1].text\n",
    "\n",
    "        nickel = pd.DataFrame(\n",
    "            data={'date': nickel_date, 'nickel': nickel_settlment}, index=[0])\n",
    "        # nickel['nickel'] = pd.to_numeric(nickel['nickel'])\n",
    "\n",
    "        return nickel\n",
    "\n",
    "    def zink():\n",
    "        url = 'https://www.westmetall.com/en/markdaten.php?action=table&field=LME_Zn_cash'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_elements = page.find_all('td')\n",
    "\n",
    "        zink_date = pd.to_datetime(page_elements[0].text)\n",
    "        zink_settlment = page_elements[1].text\n",
    "\n",
    "        zink = pd.DataFrame(\n",
    "            data={'date': zink_date, 'zink': zink_settlment}, index=[0])\n",
    "        # zink['zink'] = pd.to_numeric(zink['zink'])\n",
    "\n",
    "        return zink\n",
    "\n",
    "    al_data = aluminium()\n",
    "    cu_data = copper()\n",
    "    ld_data = lead()\n",
    "    nk_data = nickel()\n",
    "    zk_data = zink()\n",
    "\n",
    "    result = pd.merge(pd.merge(pd.merge(pd.merge(al_data, cu_data, on='date', suffixes=['_al', '_cu']),\n",
    "                                        ld_data, on='date'), nk_data, on='date'), zk_data, on='date')\n",
    "\n",
    "    result['aluminium'] = result['aluminium'].str.replace(',', '')\n",
    "    result['copper'] = result['copper'].str.replace(',', '')\n",
    "    result['lead'] = result['lead'].str.replace(',', '')\n",
    "    result['nickel'] = result['nickel'].str.replace(',', '')\n",
    "    result['zink'] = result['zink'].str.replace(',', '')\n",
    "\n",
    "    df = pd.read_excel('./data/LME_westmetall_db.xlsx', index_col=0)\n",
    "    df = pd.concat([df, result], axis=0)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/LME_westmetall_db.xlsx\",\n",
    "            date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "        df.to_excel(writer, sheet_name='LME_westmetall')\n",
    "\n",
    "    df = pd.read_excel('./data/LME_westmetall_db.xlsx', index_col=0)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        \"../parser_beta/data/LME_westmetall_db.xlsx\",\n",
    "            date_format=\"YYYY-MM-DD\",\n",
    "            datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "        df.to_excel(writer, sheet_name='LME_westmetall')\n",
    "\n",
    "    google_upload(df, 'LME_westmetall')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "##### Run!!! #####\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#    functions = {\n",
    "#        'LME': lme_db_addition, 'CB_CURR': cb_curr,\n",
    "#        'CB_METAL': cb_metall, 'NBK': nbk_tenge, 'KITCO': kitco_db\n",
    "#    }\n",
    "\n",
    "#    for number, func in tqdm(functions.items()):\n",
    "#        try:\n",
    "#            func()\n",
    "\n",
    "#        except ValueError:\n",
    "#            print(f'Supposed problems on {number} side')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lme_db_addition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "westmetall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_curr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_metall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbk_tenge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitco_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div id=48c2f727-a729-4eb3-918f-609efa4d43c5 style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands?.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('48c2f727-a729-4eb3-918f-609efa4d43c5').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-11</td>\n",
       "      <td>68690</td>\n",
       "      <td>Yuan/MT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-10</td>\n",
       "      <td>68740</td>\n",
       "      <td>Yuan/MT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-09</td>\n",
       "      <td>68710</td>\n",
       "      <td>Yuan/MT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-08</td>\n",
       "      <td>69130</td>\n",
       "      <td>Yuan/MT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-07</td>\n",
       "      <td>69290</td>\n",
       "      <td>Yuan/MT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>48930</td>\n",
       "      <td>Yuan/MT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>48950</td>\n",
       "      <td>Yuan/MT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>49060</td>\n",
       "      <td>Yuan/MT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>48990</td>\n",
       "      <td>Yuan/MT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>48605</td>\n",
       "      <td>Yuan/MT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "          date  price     unit\n",
       "0   2023-08-11  68690  Yuan/MT\n",
       "1   2023-08-10  68740  Yuan/MT\n",
       "2   2023-08-09  68710  Yuan/MT\n",
       "3   2023-08-08  69130  Yuan/MT\n",
       "4   2023-08-07  69290  Yuan/MT\n",
       "..         ...    ...      ...\n",
       "661 2020-01-17  48930  Yuan/MT\n",
       "662 2020-01-16  48950  Yuan/MT\n",
       "663 2020-01-15  49060  Yuan/MT\n",
       "664 2020-01-14  48990  Yuan/MT\n",
       "665 2020-01-10  48605  Yuan/MT\n",
       "\n",
       "[666 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shmet_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d40a6ae76423cf5fae73028028fb017d8630dd167584993d74bc99181da5a036"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
